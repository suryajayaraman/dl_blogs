{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc809fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:15:08.777830Z",
     "iopub.status.busy": "2024-08-18T18:15:08.776933Z",
     "iopub.status.idle": "2024-08-18T18:17:01.677916Z",
     "shell.execute_reply": "2024-08-18T18:17:01.675635Z"
    },
    "papermill": {
     "duration": 112.917865,
     "end_time": "2024-08-18T18:17:01.681563",
     "exception": false,
     "start_time": "2024-08-18T18:15:08.763698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\r\n",
      "Collecting torch==1.11.0+cpu\r\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-1.11.0%2Bcpu-cp310-cp310-linux_x86_64.whl (169.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.2/169.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torchvision==0.12.0+cpu\r\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.12.0%2Bcpu-cp310-cp310-linux_x86_64.whl (14.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.11.0+cpu) (4.9.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0+cpu) (1.26.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0+cpu) (2.31.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0+cpu) (9.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (2024.2.2)\r\n",
      "Installing collected packages: torch, torchvision\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.1.2+cpu\r\n",
      "    Uninstalling torch-2.1.2+cpu:\r\n",
      "      Successfully uninstalled torch-2.1.2+cpu\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.16.2+cpu\r\n",
      "    Uninstalling torchvision-0.16.2+cpu:\r\n",
      "      Successfully uninstalled torchvision-0.16.2+cpu\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pytorch-lightning 2.2.2 requires torch>=1.13.0, but you have torch 1.11.0+cpu which is incompatible.\r\n",
      "stable-baselines3 2.1.0 requires torch>=1.13, but you have torch 1.11.0+cpu which is incompatible.\r\n",
      "torchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 1.11.0+cpu which is incompatible.\r\n",
      "torchdata 0.7.1 requires torch>=2, but you have torch 1.11.0+cpu which is incompatible.\r\n",
      "torchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 1.11.0+cpu which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed torch-1.11.0+cpu torchvision-0.12.0+cpu\r\n",
      "Looking in links: https://download.openmmlab.com/mmcv/dist/cpu/torch1.11/index.html\r\n",
      "Collecting mmcv-full==1.5.3\r\n",
      "  Downloading https://download.openmmlab.com/mmcv/dist/cpu/torch1.11.0/mmcv_full-1.5.3-cp310-cp310-manylinux1_x86_64.whl (21.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting addict (from mmcv-full==1.5.3)\r\n",
      "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (21.3)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (9.5.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (6.0.1)\r\n",
      "Requirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (0.40.2)\r\n",
      "Requirement already satisfied: opencv-python>=3 in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (4.9.0.80)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->mmcv-full==1.5.3) (3.1.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->mmcv-full==1.5.3) (6.11.0)\r\n",
      "Requirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmcv-full==1.5.3) (4.2.0)\r\n",
      "Requirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmcv-full==1.5.3) (2.0.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->mmcv-full==1.5.3) (3.17.0)\r\n",
      "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\r\n",
      "Installing collected packages: addict, mmcv-full\r\n",
      "Successfully installed addict-2.4.0 mmcv-full-1.5.3\r\n",
      "Looking in links: https://download.openmmlab.com/mmdet/dist/cpu/torch1.11/index.html\r\n",
      "Collecting mmdet==2.25.0\r\n",
      "  Downloading mmdet-2.25.0-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (3.7.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (1.26.4)\r\n",
      "Collecting pycocotools (from mmdet==2.25.0)\r\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (1.16.0)\r\n",
      "Collecting terminaltables (from mmdet==2.25.0)\r\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (2.9.0.post0)\r\n",
      "Downloading mmdet-2.25.0-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: terminaltables, pycocotools, mmdet\r\n",
      "Successfully installed mmdet-2.25.0 pycocotools-2.0.8 terminaltables-3.1.10\r\n",
      "Collecting kaleido\r\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\r\n",
      "Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: kaleido\r\n",
      "Successfully installed kaleido-0.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.11.0+cpu torchvision==0.12.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install mmcv-full==1.5.3 -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.11/index.html\n",
    "!pip install mmdet==2.25.0 -f https://download.openmmlab.com/mmdet/dist/cpu/torch1.11/index.html\n",
    "!pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78ae307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:01.732748Z",
     "iopub.status.busy": "2024-08-18T18:17:01.732317Z",
     "iopub.status.idle": "2024-08-18T18:17:02.767330Z",
     "shell.execute_reply": "2024-08-18T18:17:02.766224Z"
    },
    "papermill": {
     "duration": 1.064491,
     "end_time": "2024-08-18T18:17:02.770341",
     "exception": false,
     "start_time": "2024-08-18T18:17:01.705850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "sys.path.append('/kaggle/input/transfuser-e2e-scripts')\n",
    "\n",
    "# dl imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc01a6",
   "metadata": {
    "papermill": {
     "duration": 0.024168,
     "end_time": "2024-08-18T18:17:02.820900",
     "exception": false,
     "start_time": "2024-08-18T18:17:02.796732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CARLA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0459e4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:02.870703Z",
     "iopub.status.busy": "2024-08-18T18:17:02.870123Z",
     "iopub.status.idle": "2024-08-18T18:17:03.470997Z",
     "shell.execute_reply": "2024-08-18T18:17:03.469607Z"
    },
    "papermill": {
     "duration": 0.63012,
     "end_time": "2024-08-18T18:17:03.474950",
     "exception": false,
     "start_time": "2024-08-18T18:17:02.844830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config import GlobalConfig\n",
    "from data import CARLA_Data\n",
    "\n",
    "config = GlobalConfig()\n",
    "config.pred_len = 7\n",
    "\n",
    "FPS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c8408",
   "metadata": {
    "papermill": {
     "duration": 0.025089,
     "end_time": "2024-08-18T18:17:03.525254",
     "exception": false,
     "start_time": "2024-08-18T18:17:03.500165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create pytorch style dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2b5169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:03.577008Z",
     "iopub.status.busy": "2024-08-18T18:17:03.576500Z",
     "iopub.status.idle": "2024-08-18T18:17:03.582502Z",
     "shell.execute_reply": "2024-08-18T18:17:03.580932Z"
    },
    "papermill": {
     "duration": 0.034108,
     "end_time": "2024-08-18T18:17:03.585141",
     "exception": false,
     "start_time": "2024-08-18T18:17:03.551033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03c534",
   "metadata": {
    "papermill": {
     "duration": 0.023619,
     "end_time": "2024-08-18T18:17:03.633126",
     "exception": false,
     "start_time": "2024-08-18T18:17:03.609507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed165d25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:03.684282Z",
     "iopub.status.busy": "2024-08-18T18:17:03.683865Z",
     "iopub.status.idle": "2024-08-18T18:17:23.036674Z",
     "shell.execute_reply": "2024-08-18T18:17:23.035403Z"
    },
    "papermill": {
     "duration": 19.382074,
     "end_time": "2024-08-18T18:17:23.039608",
     "exception": false,
     "start_time": "2024-08-18T18:17:03.657534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadec23ce94d4ab3a7a10eadeab5ea89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/78.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "from model import LidarCenterNet\n",
    "model = LidarCenterNet(config, device, config.backbone, image_architecture='regnety_032', \n",
    "                           lidar_architecture='regnety_032', estimate_loss=False)\n",
    "model.to(device);\n",
    "model.config.debug = True\n",
    "\n",
    "model.eval();\n",
    "checkpt = torch.load('/kaggle/input/carla-transfuser-regnet032/transfuser_regnet032_seed1_39.pth', map_location=device)\n",
    "model.load_state_dict(checkpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77beb490",
   "metadata": {
    "papermill": {
     "duration": 0.023741,
     "end_time": "2024-08-18T18:17:23.088332",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.064591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "268a0eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:23.138381Z",
     "iopub.status.busy": "2024-08-18T18:17:23.137960Z",
     "iopub.status.idle": "2024-08-18T18:17:23.147038Z",
     "shell.execute_reply": "2024-08-18T18:17:23.145856Z"
    },
    "papermill": {
     "duration": 0.037264,
     "end_time": "2024-08-18T18:17:23.149695",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.112431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import VEHICLE_TO_LIDAR_FWD, LIDAR_HEIGHT\n",
    "\n",
    "def generate_lane_points(waypoints, lane_width = 1.0):\n",
    "    # input waypoints are 2d coordinates of centerline (N,2)\n",
    "    # this function generates left and right lane corners\n",
    "    # by subtracting and adding half lane width. We convert\n",
    "    # to 3D Lidar coordinates by placing points at ground level\n",
    "\n",
    "    n_points = waypoints.shape[0]\n",
    "    lane_points = np.zeros((n_points * 2 , 3))\n",
    "    \n",
    "    # vehicle to lidar frame\n",
    "    lane_points[:n_points, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n",
    "    lane_points[n_points:, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n",
    "    \n",
    "    # left and right lanes\n",
    "    lane_points[:n_points,1] = waypoints[:,1] - (lane_width * 0.5)\n",
    "    lane_points[n_points:,1] = waypoints[:,1] + (lane_width * 0.5)\n",
    "    \n",
    "    # fixed height\n",
    "    lane_points[:,2] = -LIDAR_HEIGHT\n",
    "    return lane_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d2ce24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:23.200948Z",
     "iopub.status.busy": "2024-08-18T18:17:23.200571Z",
     "iopub.status.idle": "2024-08-18T18:17:23.210081Z",
     "shell.execute_reply": "2024-08-18T18:17:23.209130Z"
    },
    "papermill": {
     "duration": 0.038505,
     "end_time": "2024-08-18T18:17:23.212517",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.174012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bev_to_lidar = np.array([\n",
    "            [0, -(1/8.0), 32],\n",
    "            [-(1/8.0), 0, 16],\n",
    "            [0 , 0, 1]\n",
    "])\n",
    "\n",
    "\n",
    "def convert_to_3d_bboxes(boxes_2d):\n",
    "    n_boxes = boxes_2d.shape[0]\n",
    "    bbox_3d = np.zeros((n_boxes, 7))\n",
    "\n",
    "    # xy position from bev pixels to metres\n",
    "    homogenous_coordinates = np.hstack([boxes_2d[:, :2], np.ones((n_boxes, 1))])\n",
    "    bbox_3d[:, :2] = (bev_to_lidar @ homogenous_coordinates.T).T[:, :2]\n",
    "    bbox_3d[:, 3] = boxes_2d[:, 3] / 8  # length \n",
    "    bbox_3d[:, 4] = boxes_2d[:, 2] / 8  # width\n",
    "    bbox_3d[:, 6] = -boxes_2d[:, 4]      # yaw\n",
    "\n",
    "    # hardcoding z values\n",
    "    bbox_3d[:, 2] = -1.25\n",
    "    bbox_3d[:, 5] = 2.5\n",
    "    return bbox_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a793a659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:23.262854Z",
     "iopub.status.busy": "2024-08-18T18:17:23.262459Z",
     "iopub.status.idle": "2024-08-18T18:17:23.278132Z",
     "shell.execute_reply": "2024-08-18T18:17:23.276935Z"
    },
    "papermill": {
     "duration": 0.0439,
     "end_time": "2024-08-18T18:17:23.280844",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.236944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rotated_bbox(bbox):\n",
    "    x, y, w, h, yaw, _, _  =  bbox\n",
    "\n",
    "    bbox = np.array([[h,   w, 1],\n",
    "                     [h,  -w, 1],\n",
    "                     [-h, -w, 1],\n",
    "                     [-h,  w, 1],\n",
    "                ])\n",
    "    \n",
    "    # The height and width of the bounding box value was changed by this factor \n",
    "    # during data collection. Fix that for future datasets and remove    \n",
    "    bbox[:, :2] /= 2\n",
    "    bbox[:, :2] = bbox[:, [1, 0]]\n",
    "\n",
    "    c, s = np.cos(yaw), np.sin(yaw)\n",
    "    # use y x because coordinate is changed\n",
    "    r1_to_world = np.array([[c, -s, x], [s, c, y], [0, 0, 1]])\n",
    "    bbox = r1_to_world @ bbox.T\n",
    "    bbox = bbox.T\n",
    "    bbox = np.clip(bbox, 0, 256)\n",
    "    return bbox\n",
    "\n",
    "def get_scatter_plot(x,y, mode='lines', marker_size=2, color=None, **kwargs):\n",
    "    return go.Scatter(x=x, y=y, mode=mode, hoverinfo='skip',showlegend=False, \n",
    "                        marker = dict(size=marker_size, color=color), **kwargs)\n",
    "\n",
    "def plot_box_corners2d(box2d, color,**kwargs):\n",
    "    return [\n",
    "        get_scatter_plot([box2d[0,0], box2d[1,0]], [box2d[0,1], box2d[1,1]], color=color, **kwargs),\n",
    "        get_scatter_plot([box2d[1,0], box2d[2,0]], [box2d[1,1], box2d[2,1]], color=color, **kwargs),\n",
    "        get_scatter_plot([box2d[2,0], box2d[3,0]], [box2d[2,1], box2d[3,1]], color=color, **kwargs),\n",
    "        get_scatter_plot([box2d[3,0], box2d[0,0]], [box2d[3,1], box2d[0,1]], color=color, **kwargs),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc4092",
   "metadata": {
    "papermill": {
     "duration": 0.024294,
     "end_time": "2024-08-18T18:17:23.330437",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.306143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6dd58c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:23.384663Z",
     "iopub.status.busy": "2024-08-18T18:17:23.383785Z",
     "iopub.status.idle": "2024-08-18T18:17:23.391231Z",
     "shell.execute_reply": "2024-08-18T18:17:23.389804Z"
    },
    "papermill": {
     "duration": 0.035824,
     "end_time": "2024-08-18T18:17:23.394112",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.358288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PCD_CAM_VIEW = dict(\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            eye=dict(x=-0.9, y=0, z=0.2)\n",
    "    )\n",
    "\n",
    "PCD_SCENE=dict(\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        zaxis=dict(visible=False,),\n",
    "        aspectmode='manual',\n",
    "        aspectratio=dict(x=1, y=1, z=0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434507ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:23.446758Z",
     "iopub.status.busy": "2024-08-18T18:17:23.445651Z",
     "iopub.status.idle": "2024-08-18T18:17:23.602917Z",
     "shell.execute_reply": "2024-08-18T18:17:23.601385Z"
    },
    "papermill": {
     "duration": 0.186565,
     "end_time": "2024-08-18T18:17:23.605944",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.419379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_lidar3d_plots, get_image2d_plots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "class Visualizer:\n",
    "    def __init__(self, model_name, fig_width=1000, fig_height=800, pred_box_color='orange', \n",
    "                 waypoints_color = 'red', bbox_2d_color = 'cyan', scene=PCD_SCENE, cam_view=PCD_CAM_VIEW):\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Create a 2x3 figure, top row for point cloud data\n",
    "        # bottom row for rgb image data\n",
    "        self.fig = make_subplots(rows=3, cols=2,\n",
    "                                 specs=[[{\"type\": \"scatter3d\", \"colspan\": 2}, None], \n",
    "                                        [{}, {\"rowspan\": 2}],\n",
    "                                        [{}, None]], \n",
    "                                row_heights=[0.6, 0.2, 0.2], horizontal_spacing=0.0, vertical_spacing = 0.0)\n",
    "        \n",
    "        self.fig.update_layout(template=\"plotly_dark\", scene=scene, scene_camera = cam_view,\n",
    "                height = fig_height, width = fig_width, autosize=False,\n",
    "                title=f\"END TO END AUTONOMOUS DRIVING {self.model_name}\", title_x=0.5, title_y=0.95,\n",
    "                margin=dict(r=0, b=0, l=0, t=0))\n",
    "        for row in range(2,4):\n",
    "            for col in range(1,3):\n",
    "                self.fig.update_xaxes(showticklabels=False, visible=False, row=row, col=col)\n",
    "                self.fig.update_yaxes(showticklabels=False, visible=False, row=row, col=col)\n",
    "        \n",
    "        # set export image option\n",
    "        self.fig.to_image(format=\"png\", engine=\"kaleido\")\n",
    "        self.pred_color = pred_box_color\n",
    "        self.waypoints_color = waypoints_color\n",
    "        self.box2d_color = bbox_2d_color\n",
    "\n",
    "    def clear_figure_data(self):\n",
    "        self.fig.data = []\n",
    "    \n",
    "    def get_bbox_colors(self, bbox_corners):\n",
    "        return [self.pred_color] * bbox_corners.shape[0] if bbox_corners is not None else None\n",
    "        \n",
    "    def plot_waypoints(self, waypoints):\n",
    "        return go.Mesh3d(x=waypoints[:,0], y=waypoints[:,1], z=waypoints[:,2], \n",
    "                         opacity=0.4, color=self.waypoints_color, \n",
    "                         hoverinfo='skip',showlegend=False)\n",
    "\n",
    "    def add_lidar_plots(self, points, waypoints, pred_corners=None):\n",
    "        lidar_3d_plots = get_lidar3d_plots(points, pc_kwargs=dict(colorscale='viridis', marker_size=0.9),\n",
    "                                   pred_box_corners = pred_corners, \n",
    "                                   pred_box_colors = self.get_bbox_colors(pred_corners))\n",
    "        lidar_3d_plots.append(self.plot_waypoints(waypoints))\n",
    "        for trace in lidar_3d_plots:\n",
    "            self.fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "    def add_image_plots(self, rgb_image, depth_image, lidar_data, pred_corners_2d):\n",
    "        self.fig.add_trace(get_image2d_plots(rgb_image), row=2, col=1)\n",
    "        \n",
    "        # repeating depth image 3 times to get standard channel\n",
    "        depth_image = np.tile(depth_image[:, :, None], (1,1,3))\n",
    "        self.fig.add_trace(get_image2d_plots(depth_image), row=3, col=1)\n",
    "        \n",
    "        # BEV lidar image with bounding boxes\n",
    "        self.fig.add_trace(get_image2d_plots(lidar_data), row=2, col=2)\n",
    "        box_colors = [self.box2d_color] * len(pred_corners_2d)\n",
    "        for i, obj_i in enumerate(pred_corners_2d):\n",
    "            obj_plots = plot_box_corners2d(obj_i, color = box_colors[i])\n",
    "            for plot in obj_plots:\n",
    "                self.fig.add_trace(plot, row=2, col=2)\n",
    "        \n",
    "    def visualize_predictions(self, points, waypoints, pred_corners_3d, \n",
    "                              rgb_image, depth_image, lidar_data, pred_corners_2d):\n",
    "        # clear previous data and plot lidar, image data\n",
    "        self.clear_figure_data()\n",
    "        self.add_lidar_plots(points=points, waypoints=waypoints, pred_corners=pred_corners_3d)\n",
    "        self.add_image_plots(rgb_image, depth_image, lidar_data, pred_corners_2d)\n",
    "    \n",
    "    def show_figure(self):\n",
    "        self.fig.show()\n",
    "        \n",
    "    def save_to_png(self, output_path):\n",
    "        self.fig.write_image(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7d90b",
   "metadata": {
    "papermill": {
     "duration": 0.025129,
     "end_time": "2024-08-18T18:17:23.655988",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.630859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "722e7a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:23.707375Z",
     "iopub.status.busy": "2024-08-18T18:17:23.706440Z",
     "iopub.status.idle": "2024-08-18T18:17:23.722727Z",
     "shell.execute_reply": "2024-08-18T18:17:23.721693Z"
    },
    "papermill": {
     "duration": 0.045178,
     "end_time": "2024-08-18T18:17:23.725582",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.680404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Eigen CAM imports\n",
    "from e2e_cam import EigenCAM, PlannerTarget, show_cam_on_image\n",
    "\n",
    "# which layers should be focus on\n",
    "target_layers = [model._model.image_encoder.features.layer4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b466090e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:23.777966Z",
     "iopub.status.busy": "2024-08-18T18:17:23.777579Z",
     "iopub.status.idle": "2024-08-18T18:17:23.796256Z",
     "shell.execute_reply": "2024-08-18T18:17:23.794854Z"
    },
    "papermill": {
     "duration": 0.048222,
     "end_time": "2024-08-18T18:17:23.798624",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.750402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import boxes_to_corners_3d\n",
    "\n",
    "def write_model_predictions(dataloader_demo, output_dir):\n",
    "    frameIdx = 0\n",
    "        \n",
    "    with EigenCAM(model=model, target_layers=target_layers) as cam:    \n",
    "        for data in tqdm(dataloader_demo):\n",
    "            # load data to device, according to type\n",
    "            for k in ['rgb', 'depth', 'lidar', 'label', 'ego_waypoint', \\\n",
    "                      'target_point', 'target_point_image', 'speed']:\n",
    "                data[k] = data[k].to(device, torch.float32)\n",
    "            for k in ['semantic', 'bev']:\n",
    "                data[k] = data[k].to(device, torch.long)\n",
    "\n",
    "            # get model predictions\n",
    "            targets = [PlannerTarget(data['ego_waypoint'])]            \n",
    "            grayscale_cam = cam(input_data=data, targets=targets, key=['rgb'])\n",
    "            outputs = cam.outputs[1]\n",
    "\n",
    "            # iterate through each sample in batch \n",
    "            bs = data['rgb'].shape[0]\n",
    "            for i in range(bs):\n",
    "                # input data\n",
    "                rgb_image = data['rgb'][i].permute(1, 2, 0).detach().cpu().numpy()\n",
    "                tgt_waypoints = data['ego_waypoint'][i].detach().cpu().numpy()\n",
    "                lidar_pc = data['raw_lidar'][i].detach().cpu().numpy()\n",
    "                num_points = data['num_raw_lidar_points'].detach().cpu().numpy()[i]\n",
    "                lidar_pc = lidar_pc[:num_points, :]\n",
    "\n",
    "                # bev lidar image\n",
    "                lidar_data = data['lidar'][i].detach().cpu().numpy().transpose(1,2,0)\n",
    "                lidar_data = (lidar_data * 255).astype(np.uint8)\n",
    "\n",
    "                # MODEL PREDICTIONS\n",
    "                pred_waypoints = outputs['pred_wp'][i]\n",
    "                pred_waypoints[:,1] *= -1\n",
    "                pred_lanepoints = generate_lane_points(pred_waypoints)\n",
    "\n",
    "                ## AUXILLARY TASK PREDICTIONS\n",
    "\n",
    "                ## bounding boxes\n",
    "                pred_boxes = outputs['detections'][i]\n",
    "                pred_3d_boxes = convert_to_3d_bboxes(pred_boxes)\n",
    "                pred_corners_3d = boxes_to_corners_3d(pred_3d_boxes)\n",
    "\n",
    "                # project to bev image space\n",
    "                pred_corners_2d = [get_rotated_bbox(bbox)[:, :2] for bbox in pred_boxes]\n",
    "\n",
    "                ## depth information\n",
    "                pred_depth = (outputs['pred_depth'][i] * 255).astype(np.uint8)\n",
    "                pred_bev = outputs['pred_bev'][i].argmax(axis=0).astype(np.uint8)\n",
    "                \n",
    "                # class activation maps\n",
    "                rgb_image = rgb_image / 255.0\n",
    "                cam_image = show_cam_on_image(rgb_image, grayscale_cam[0][i, 0], use_rgb=True)\n",
    "                    \n",
    "                # plot all data\n",
    "                visualizer.visualize_predictions(lidar_pc, pred_lanepoints, pred_corners_3d,\n",
    "                                                 cam_image, pred_depth, lidar_data, \n",
    "                                                 pred_corners_2d = pred_corners_2d)\n",
    "                # save figure\n",
    "                visualizer.save_to_png(os.path.join(output_dir, f\"Frame{frameIdx}.png\"))\n",
    "                frameIdx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d307897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:23.851236Z",
     "iopub.status.busy": "2024-08-18T18:17:23.850420Z",
     "iopub.status.idle": "2024-08-18T18:17:23.861288Z",
     "shell.execute_reply": "2024-08-18T18:17:23.859705Z"
    },
    "papermill": {
     "duration": 0.040926,
     "end_time": "2024-08-18T18:17:23.864196",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.823270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_images_to_video(images_dir, output_video_path, fps : int = 8):\n",
    "    input_images = [os.path.join(images_dir, *[x]) for x in sorted(os.listdir(images_dir)) if x.endswith('png')]\n",
    "    \n",
    "    if(len(input_images) > 0):\n",
    "        sample_image = cv2.imread(input_images[0])\n",
    "        height, width, _ = sample_image.shape\n",
    "        \n",
    "        # handles for input output videos\n",
    "        output_handle = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "\n",
    "        # create progress bar\n",
    "        num_frames = int(len(input_images))\n",
    "        pbar = tqdm(total = num_frames, position=0, leave=True)\n",
    "\n",
    "        for i in tqdm(range(num_frames), position=0, leave=True):\n",
    "            frame = cv2.imread(input_images[i])\n",
    "            output_handle.write(frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "        # release the output video handler\n",
    "        output_handle.release()\n",
    "                \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b2a48b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:23.915928Z",
     "iopub.status.busy": "2024-08-18T18:17:23.915157Z",
     "iopub.status.idle": "2024-08-18T18:17:23.921544Z",
     "shell.execute_reply": "2024-08-18T18:17:23.920355Z"
    },
    "papermill": {
     "duration": 0.034611,
     "end_time": "2024-08-18T18:17:23.924160",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.889549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_route_name(folder):\n",
    "    contains_routes = [x for x in folder.split('_') if 'route' in x and len(x)>=6]\n",
    "    if 'routes' in contains_routes:\n",
    "        contains_routes.remove('routes')\n",
    "    return contains_routes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab2e0aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T18:17:24.049768Z",
     "iopub.status.busy": "2024-08-18T18:17:24.049316Z",
     "iopub.status.idle": "2024-08-18T21:53:43.533946Z",
     "shell.execute_reply": "2024-08-18T21:53:43.532408Z"
    },
    "papermill": {
     "duration": 12979.588257,
     "end_time": "2024-08-18T21:53:43.537128",
     "exception": false,
     "start_time": "2024-08-18T18:17:23.948871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.74it/s]\n",
      "There are 86 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/mmdet/models/utils/gaussian_target.py:227: UserWarning:\n",
      "\n",
      "__floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/mmdet/models/utils/gaussian_target.py:229: UserWarning:\n",
      "\n",
      "__floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "\n",
      "100%|██████████| 43/43 [07:39<00:00, 10.68s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 86/86 [00:01<00:00, 51.26it/s]\n",
      "100%|██████████| 86/86 [00:01<00:00, 51.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route0 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 33.91it/s]\n",
      "There are 66 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [05:46<00:00, 10.49s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 66/66 [00:01<00:00, 50.37it/s]\n",
      "100%|██████████| 66/66 [00:01<00:00, 50.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route10 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.27it/s]\n",
      "There are 67 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [06:06<00:00, 10.78s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 67/67 [00:01<00:00, 52.89it/s]\n",
      "100%|██████████| 67/67 [00:01<00:00, 52.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route11 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 48.48it/s]\n",
      "There are 44 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [03:55<00:00, 10.71s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 44/44 [00:01<00:00, 40.56it/s]\n",
      "100%|██████████| 44/44 [00:01<00:00, 40.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route12 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.68it/s]\n",
      "There are 56 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [05:21<00:00, 11.49s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 56/56 [00:01<00:00, 39.63it/s]\n",
      "100%|██████████| 56/56 [00:01<00:00, 39.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route13 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.16it/s]\n",
      "There are 73 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [06:58<00:00, 11.32s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 73/73 [00:01<00:00, 52.15it/s]\n",
      "100%|██████████| 73/73 [00:01<00:00, 52.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route14 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.90it/s]\n",
      "There are 63 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [06:15<00:00, 11.74s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 63/63 [00:01<00:00, 52.67it/s]\n",
      "100%|██████████| 63/63 [00:01<00:00, 52.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route15 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.69it/s]\n",
      "There are 43 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [04:23<00:00, 11.96s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 43/43 [00:00<00:00, 48.64it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 48.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route16 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.11it/s]\n",
      "There are 80 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [08:13<00:00, 12.35s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 80/80 [00:02<00:00, 34.45it/s]\n",
      "100%|██████████| 80/80 [00:02<00:00, 34.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route17 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 43.19it/s]\n",
      "There are 41 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [04:06<00:00, 11.72s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 41/41 [00:00<00:00, 51.65it/s]\n",
      "100%|██████████| 41/41 [00:00<00:00, 51.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route18 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.69it/s]\n",
      "There are 40 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:12<00:00, 12.62s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 40/40 [00:01<00:00, 33.97it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 33.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route19 completed ...\n",
      "100%|██████████| 11/11 [00:00<00:00, 39.07it/s]\n",
      "There are 624 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [1:11:17<00:00, 13.71s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 624/624 [00:17<00:00, 36.21it/s]\n",
      "100%|██████████| 624/624 [00:17<00:00, 36.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route1 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.29it/s]\n",
      "There are 42 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [05:28<00:00, 15.63s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 42/42 [00:01<00:00, 34.53it/s]\n",
      "100%|██████████| 42/42 [00:01<00:00, 33.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route20 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.94it/s]\n",
      "There are 43 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [05:15<00:00, 14.32s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 43/43 [00:00<00:00, 53.05it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 52.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route21 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 45.41it/s]\n",
      "There are 36 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [03:47<00:00, 12.63s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 36/36 [00:00<00:00, 51.48it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 51.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route22 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.35it/s]\n",
      "There are 32 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:45<00:00, 10.32s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 32/32 [00:00<00:00, 52.42it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 52.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route23 completed ...\n",
      "100%|██████████| 5/5 [00:00<00:00, 77.89it/s]\n",
      "There are 211 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [18:56<00:00, 10.72s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 211/211 [00:03<00:00, 52.75it/s]\n",
      "100%|██████████| 211/211 [00:04<00:00, 52.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route2 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.58it/s]\n",
      "There are 68 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [06:19<00:00, 11.16s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 68/68 [00:01<00:00, 45.32it/s]\n",
      "100%|██████████| 68/68 [00:01<00:00, 45.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route3 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.72it/s]\n",
      "There are 60 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [05:29<00:00, 10.98s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 60/60 [00:01<00:00, 42.03it/s]\n",
      "100%|██████████| 60/60 [00:01<00:00, 41.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route4 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.38it/s]\n",
      "There are 129 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [12:23<00:00, 11.45s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 129/129 [00:02<00:00, 53.27it/s]\n",
      "100%|██████████| 129/129 [00:02<00:00, 53.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route5 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 29.22it/s]\n",
      "There are 43 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [04:16<00:00, 11.68s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.97it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route6 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.17it/s]\n",
      "There are 41 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [03:55<00:00, 11.23s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 41/41 [00:00<00:00, 51.99it/s]\n",
      "100%|██████████| 41/41 [00:00<00:00, 51.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route7 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.16it/s]\n",
      "There are 66 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [06:38<00:00, 12.08s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 66/66 [00:02<00:00, 32.10it/s]\n",
      "100%|██████████| 66/66 [00:02<00:00, 31.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route8 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.27it/s]\n",
      "There are 56 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [05:50<00:00, 12.52s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 56/56 [00:01<00:00, 31.57it/s]\n",
      "100%|██████████| 56/56 [00:01<00:00, 31.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route9 completed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "visualizer = Visualizer(model_name='TRANSFUSER')\n",
    "\n",
    "root_dir = '/kaggle/input/carla-e2e-driving-data-2'\n",
    "scenarios = sorted(os.listdir(root_dir))\n",
    "\n",
    "for scenario in scenarios:\n",
    "    scenario_dir = os.path.join(root_dir, scenario)\n",
    "    routes = sorted(os.listdir(scenario_dir))\n",
    "    \n",
    "    for route in routes:\n",
    "        route_name = get_route_name(route)\n",
    "        route_dir = os.path.join(scenario_dir, route)\n",
    "        demo_set = CARLA_Data(root=scenario_dir, config=config, routeKey=route_name, load_raw_lidar=True)\n",
    "        print(f\"There are {len(demo_set)} samples in Demo dataset\")\n",
    "        dataloader_demo = DataLoader(demo_set, shuffle=False, batch_size=2, num_workers=4)\n",
    "        \n",
    "        # create output directory for scenario\n",
    "        output_dir = os.path.join(os.getcwd(), *[scenario, route_name])\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        \n",
    "        write_model_predictions(dataloader_demo, output_dir)\n",
    "        convert_images_to_video(output_dir, f'{scenario}_{route_name}_{model.pred_len}pts_{FPS}fps.mp4', fps=FPS)\n",
    "        print(f\"{scenario} {route_name} completed ...\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4922400,
     "sourceId": 8287377,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4849261,
     "sourceId": 9198582,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5559861,
     "sourceId": 9196580,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13121.482141,
   "end_time": "2024-08-18T21:53:46.720475",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-18T18:15:05.238334",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "002476163ee347c0b02242035df43a86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e06665ce4ac44b29ea46f72861fdbd9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f295a9f73924214af1bf495f5582dae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4f0718b8c8e64abcbc294f355c60e3e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "722af1e053f547e4ab7a07ed0fe1b81e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d48ec548842a4c15859dbd31d53eca81",
       "max": 78055620.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4f0718b8c8e64abcbc294f355c60e3e2",
       "value": 78055620.0
      }
     },
     "979708c674e94559994f0e0672d1ec40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0e06665ce4ac44b29ea46f72861fdbd9",
       "placeholder": "​",
       "style": "IPY_MODEL_3f295a9f73924214af1bf495f5582dae",
       "value": "model.safetensors: 100%"
      }
     },
     "aadec23ce94d4ab3a7a10eadeab5ea89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_979708c674e94559994f0e0672d1ec40",
        "IPY_MODEL_722af1e053f547e4ab7a07ed0fe1b81e",
        "IPY_MODEL_dae6f3aefdbd49ad90aaf5249f9a9c36"
       ],
       "layout": "IPY_MODEL_cc5201a50c704d389f2690c4aeef12f0"
      }
     },
     "cc5201a50c704d389f2690c4aeef12f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d48ec548842a4c15859dbd31d53eca81": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dae6f3aefdbd49ad90aaf5249f9a9c36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_002476163ee347c0b02242035df43a86",
       "placeholder": "​",
       "style": "IPY_MODEL_fd522fe416bf4c7e80014d6af181a8b1",
       "value": " 78.1M/78.1M [00:01&lt;00:00, 56.1MB/s]"
      }
     },
     "fd522fe416bf4c7e80014d6af181a8b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
