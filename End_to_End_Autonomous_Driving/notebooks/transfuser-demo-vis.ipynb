{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e79fc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:53:43.202293Z",
     "iopub.status.busy": "2024-08-25T17:53:43.201925Z",
     "iopub.status.idle": "2024-08-25T17:55:17.621509Z",
     "shell.execute_reply": "2024-08-25T17:55:17.620344Z"
    },
    "papermill": {
     "duration": 94.429701,
     "end_time": "2024-08-25T17:55:17.624313",
     "exception": false,
     "start_time": "2024-08-25T17:53:43.194612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\r\n",
      "Collecting torch==1.11.0+cpu\r\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-1.11.0%2Bcpu-cp310-cp310-linux_x86_64.whl (169.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.2/169.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torchvision==0.12.0+cpu\r\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.12.0%2Bcpu-cp310-cp310-linux_x86_64.whl (14.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.11.0+cpu) (4.9.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0+cpu) (1.26.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0+cpu) (2.31.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0+cpu) (9.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (2024.2.2)\r\n",
      "Installing collected packages: torch, torchvision\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.1.2+cpu\r\n",
      "    Uninstalling torch-2.1.2+cpu:\r\n",
      "      Successfully uninstalled torch-2.1.2+cpu\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.16.2+cpu\r\n",
      "    Uninstalling torchvision-0.16.2+cpu:\r\n",
      "      Successfully uninstalled torchvision-0.16.2+cpu\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pytorch-lightning 2.2.2 requires torch>=1.13.0, but you have torch 1.11.0+cpu which is incompatible.\r\n",
      "stable-baselines3 2.1.0 requires torch>=1.13, but you have torch 1.11.0+cpu which is incompatible.\r\n",
      "torchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 1.11.0+cpu which is incompatible.\r\n",
      "torchdata 0.7.1 requires torch>=2, but you have torch 1.11.0+cpu which is incompatible.\r\n",
      "torchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 1.11.0+cpu which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed torch-1.11.0+cpu torchvision-0.12.0+cpu\r\n",
      "Looking in links: https://download.openmmlab.com/mmcv/dist/cpu/torch1.11/index.html\r\n",
      "Collecting mmcv-full==1.5.3\r\n",
      "  Downloading https://download.openmmlab.com/mmcv/dist/cpu/torch1.11.0/mmcv_full-1.5.3-cp310-cp310-manylinux1_x86_64.whl (21.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting addict (from mmcv-full==1.5.3)\r\n",
      "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (21.3)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (9.5.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (6.0.1)\r\n",
      "Requirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (0.40.2)\r\n",
      "Requirement already satisfied: opencv-python>=3 in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (4.9.0.80)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->mmcv-full==1.5.3) (3.1.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->mmcv-full==1.5.3) (6.11.0)\r\n",
      "Requirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmcv-full==1.5.3) (4.2.0)\r\n",
      "Requirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmcv-full==1.5.3) (2.0.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->mmcv-full==1.5.3) (3.17.0)\r\n",
      "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\r\n",
      "Installing collected packages: addict, mmcv-full\r\n",
      "Successfully installed addict-2.4.0 mmcv-full-1.5.3\r\n",
      "Looking in links: https://download.openmmlab.com/mmdet/dist/cpu/torch1.11/index.html\r\n",
      "Collecting mmdet==2.25.0\r\n",
      "  Downloading mmdet-2.25.0-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (3.7.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (1.26.4)\r\n",
      "Collecting pycocotools (from mmdet==2.25.0)\r\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (1.16.0)\r\n",
      "Collecting terminaltables (from mmdet==2.25.0)\r\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (2.9.0.post0)\r\n",
      "Downloading mmdet-2.25.0-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: terminaltables, pycocotools, mmdet\r\n",
      "Successfully installed mmdet-2.25.0 pycocotools-2.0.8 terminaltables-3.1.10\r\n",
      "Collecting kaleido\r\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\r\n",
      "Collecting natsort\r\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\r\n",
      "Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading natsort-8.4.0-py3-none-any.whl (38 kB)\r\n",
      "Installing collected packages: kaleido, natsort\r\n",
      "Successfully installed kaleido-0.2.1 natsort-8.4.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.11.0+cpu torchvision==0.12.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install mmcv-full==1.5.3 -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.11/index.html\n",
    "!pip install mmdet==2.25.0 -f https://download.openmmlab.com/mmdet/dist/cpu/torch1.11/index.html\n",
    "!pip install kaleido natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58bf9699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:17.659230Z",
     "iopub.status.busy": "2024-08-25T17:55:17.658851Z",
     "iopub.status.idle": "2024-08-25T17:55:18.515635Z",
     "shell.execute_reply": "2024-08-25T17:55:18.514693Z"
    },
    "papermill": {
     "duration": 0.87671,
     "end_time": "2024-08-25T17:55:18.518270",
     "exception": false,
     "start_time": "2024-08-25T17:55:17.641560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "sys.path.append('/kaggle/input/transfuser-e2e-scripts')\n",
    "\n",
    "# dl imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612a82f",
   "metadata": {
    "papermill": {
     "duration": 0.01476,
     "end_time": "2024-08-25T17:55:18.549193",
     "exception": false,
     "start_time": "2024-08-25T17:55:18.534433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CARLA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812c6474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:18.582407Z",
     "iopub.status.busy": "2024-08-25T17:55:18.581393Z",
     "iopub.status.idle": "2024-08-25T17:55:19.054705Z",
     "shell.execute_reply": "2024-08-25T17:55:19.053829Z"
    },
    "papermill": {
     "duration": 0.492507,
     "end_time": "2024-08-25T17:55:19.057083",
     "exception": false,
     "start_time": "2024-08-25T17:55:18.564576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config import GlobalConfig\n",
    "from data import CARLA_Data\n",
    "\n",
    "config = GlobalConfig()\n",
    "config.pred_len = 7\n",
    "\n",
    "FPS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cf75f",
   "metadata": {
    "papermill": {
     "duration": 0.014402,
     "end_time": "2024-08-25T17:55:19.086332",
     "exception": false,
     "start_time": "2024-08-25T17:55:19.071930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create pytorch style dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c2941d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:19.117689Z",
     "iopub.status.busy": "2024-08-25T17:55:19.116832Z",
     "iopub.status.idle": "2024-08-25T17:55:19.121426Z",
     "shell.execute_reply": "2024-08-25T17:55:19.120447Z"
    },
    "papermill": {
     "duration": 0.022236,
     "end_time": "2024-08-25T17:55:19.123306",
     "exception": false,
     "start_time": "2024-08-25T17:55:19.101070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e781582",
   "metadata": {
    "papermill": {
     "duration": 0.01424,
     "end_time": "2024-08-25T17:55:19.152156",
     "exception": false,
     "start_time": "2024-08-25T17:55:19.137916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8780122a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:19.183386Z",
     "iopub.status.busy": "2024-08-25T17:55:19.182566Z",
     "iopub.status.idle": "2024-08-25T17:55:41.724475Z",
     "shell.execute_reply": "2024-08-25T17:55:41.723533Z"
    },
    "papermill": {
     "duration": 22.560083,
     "end_time": "2024-08-25T17:55:41.726838",
     "exception": false,
     "start_time": "2024-08-25T17:55:19.166755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458b934130764270a39d43f7f87a254e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/78.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "from model import LidarCenterNet\n",
    "model = LidarCenterNet(config, device, config.backbone, image_architecture='regnety_032', \n",
    "                           lidar_architecture='regnety_032', estimate_loss=False)\n",
    "model.to(device);\n",
    "model.config.debug = True\n",
    "\n",
    "model.eval();\n",
    "checkpt = torch.load('/kaggle/input/carla-transfuser-regnet032/transfuser_regnet032_seed1_39.pth', map_location=device)\n",
    "model.load_state_dict(checkpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e483e",
   "metadata": {
    "papermill": {
     "duration": 0.01477,
     "end_time": "2024-08-25T17:55:41.757043",
     "exception": false,
     "start_time": "2024-08-25T17:55:41.742273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca7b0d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:41.789591Z",
     "iopub.status.busy": "2024-08-25T17:55:41.788581Z",
     "iopub.status.idle": "2024-08-25T17:55:41.795906Z",
     "shell.execute_reply": "2024-08-25T17:55:41.794839Z"
    },
    "papermill": {
     "duration": 0.025958,
     "end_time": "2024-08-25T17:55:41.798070",
     "exception": false,
     "start_time": "2024-08-25T17:55:41.772112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import VEHICLE_TO_LIDAR_FWD, LIDAR_HEIGHT\n",
    "\n",
    "def generate_lane_points(waypoints, lane_width = 1.0):\n",
    "    # input waypoints are 2d coordinates of centerline (N,2)\n",
    "    # this function generates left and right lane corners\n",
    "    # by subtracting and adding half lane width. We convert\n",
    "    # to 3D Lidar coordinates by placing points at ground level\n",
    "\n",
    "    n_points = waypoints.shape[0]\n",
    "    lane_points = np.zeros((n_points * 2 , 3))\n",
    "    \n",
    "    # vehicle to lidar frame\n",
    "    lane_points[:n_points, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n",
    "    lane_points[n_points:, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n",
    "    \n",
    "    # left and right lanes\n",
    "    lane_points[:n_points,1] = waypoints[:,1] - (lane_width * 0.5)\n",
    "    lane_points[n_points:,1] = waypoints[:,1] + (lane_width * 0.5)\n",
    "    \n",
    "    # fixed height\n",
    "    lane_points[:,2] = -LIDAR_HEIGHT\n",
    "    return lane_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a76dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:41.830373Z",
     "iopub.status.busy": "2024-08-25T17:55:41.829430Z",
     "iopub.status.idle": "2024-08-25T17:55:41.837552Z",
     "shell.execute_reply": "2024-08-25T17:55:41.836486Z"
    },
    "papermill": {
     "duration": 0.026516,
     "end_time": "2024-08-25T17:55:41.839812",
     "exception": false,
     "start_time": "2024-08-25T17:55:41.813296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bev_to_lidar = np.array([\n",
    "            [0, -(1/8.0), 32],\n",
    "            [-(1/8.0), 0, 16],\n",
    "            [0 , 0, 1]\n",
    "])\n",
    "\n",
    "\n",
    "def convert_to_3d_bboxes(boxes_2d):\n",
    "    n_boxes = boxes_2d.shape[0]\n",
    "    bbox_3d = np.zeros((n_boxes, 7))\n",
    "\n",
    "    # xy position from bev pixels to metres\n",
    "    homogenous_coordinates = np.hstack([boxes_2d[:, :2], np.ones((n_boxes, 1))])\n",
    "    bbox_3d[:, :2] = (bev_to_lidar @ homogenous_coordinates.T).T[:, :2]\n",
    "    bbox_3d[:, 3] = boxes_2d[:, 3] / 8  # length \n",
    "    bbox_3d[:, 4] = boxes_2d[:, 2] / 8  # width\n",
    "    bbox_3d[:, 6] = -boxes_2d[:, 4]      # yaw\n",
    "\n",
    "    # hardcoding z values\n",
    "    bbox_3d[:, 2] = -1.25\n",
    "    bbox_3d[:, 5] = 2.5\n",
    "    return bbox_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604f42ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:41.872494Z",
     "iopub.status.busy": "2024-08-25T17:55:41.871561Z",
     "iopub.status.idle": "2024-08-25T17:55:41.882807Z",
     "shell.execute_reply": "2024-08-25T17:55:41.881724Z"
    },
    "papermill": {
     "duration": 0.029769,
     "end_time": "2024-08-25T17:55:41.884866",
     "exception": false,
     "start_time": "2024-08-25T17:55:41.855097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rotated_bbox(bbox):\n",
    "    x, y, w, h, yaw, _, _  =  bbox\n",
    "\n",
    "    bbox = np.array([[h,   w, 1],\n",
    "                     [h,  -w, 1],\n",
    "                     [-h, -w, 1],\n",
    "                     [-h,  w, 1],\n",
    "                ])\n",
    "    \n",
    "    # The height and width of the bounding box value was changed by this factor \n",
    "    # during data collection. Fix that for future datasets and remove    \n",
    "    bbox[:, :2] /= 2\n",
    "    bbox[:, :2] = bbox[:, [1, 0]]\n",
    "\n",
    "    c, s = np.cos(yaw), np.sin(yaw)\n",
    "    # use y x because coordinate is changed\n",
    "    r1_to_world = np.array([[c, -s, x], [s, c, y], [0, 0, 1]])\n",
    "    bbox = r1_to_world @ bbox.T\n",
    "    bbox = bbox.T\n",
    "    bbox = np.clip(bbox, 0, 256)\n",
    "    return bbox\n",
    "\n",
    "def get_scatter_plot(x,y, mode='lines', marker_size=2, color=None, **kwargs):\n",
    "    return go.Scatter(x=x, y=y, mode=mode, hoverinfo='skip',showlegend=False, \n",
    "                        marker = dict(size=marker_size, color=color), **kwargs)\n",
    "\n",
    "def plot_box_corners2d(box2d, color,**kwargs):\n",
    "    return [\n",
    "        get_scatter_plot([box2d[0,0], box2d[1,0]], [box2d[0,1], box2d[1,1]], color=color, **kwargs),\n",
    "        get_scatter_plot([box2d[1,0], box2d[2,0]], [box2d[1,1], box2d[2,1]], color=color, **kwargs),\n",
    "        get_scatter_plot([box2d[2,0], box2d[3,0]], [box2d[2,1], box2d[3,1]], color=color, **kwargs),\n",
    "        get_scatter_plot([box2d[3,0], box2d[0,0]], [box2d[3,1], box2d[0,1]], color=color, **kwargs),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48335ad1",
   "metadata": {
    "papermill": {
     "duration": 0.015104,
     "end_time": "2024-08-25T17:55:41.915521",
     "exception": false,
     "start_time": "2024-08-25T17:55:41.900417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad37f829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:41.947787Z",
     "iopub.status.busy": "2024-08-25T17:55:41.947031Z",
     "iopub.status.idle": "2024-08-25T17:55:41.952882Z",
     "shell.execute_reply": "2024-08-25T17:55:41.951910Z"
    },
    "papermill": {
     "duration": 0.024269,
     "end_time": "2024-08-25T17:55:41.955004",
     "exception": false,
     "start_time": "2024-08-25T17:55:41.930735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PCD_CAM_VIEW = dict(\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            eye=dict(x=-0.9, y=0, z=0.2)\n",
    "    )\n",
    "\n",
    "PCD_SCENE=dict(\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        zaxis=dict(visible=False,),\n",
    "        aspectmode='manual',\n",
    "        aspectratio=dict(x=1, y=1, z=0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f31333a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:41.987309Z",
     "iopub.status.busy": "2024-08-25T17:55:41.986935Z",
     "iopub.status.idle": "2024-08-25T17:55:42.118212Z",
     "shell.execute_reply": "2024-08-25T17:55:42.117324Z"
    },
    "papermill": {
     "duration": 0.150409,
     "end_time": "2024-08-25T17:55:42.120677",
     "exception": false,
     "start_time": "2024-08-25T17:55:41.970268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_lidar3d_plots, get_image2d_plots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "class Visualizer:\n",
    "    def __init__(self, model_name, fig_width=1000, fig_height=800, pred_box_color='white', \n",
    "                 waypoints_color = 'red', bbox_2d_color = 'cyan', scene=PCD_SCENE, \n",
    "                 cam_view=PCD_CAM_VIEW):\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Create a 2x3 figure, top row for point cloud data\n",
    "        # bottom row for rgb image data\n",
    "        self.fig = make_subplots(rows=3, cols=2,\n",
    "                                 specs=[[{\"type\": \"scatter3d\", \"colspan\": 2}, None], \n",
    "                                        [{}, {\"rowspan\": 2}],\n",
    "                                        [{}, None]], \n",
    "                                row_heights=[0.6, 0.2, 0.2], horizontal_spacing=0.0, vertical_spacing = 0.0)\n",
    "        \n",
    "        self.fig.update_layout(template=\"plotly_dark\", scene=scene, scene_camera = cam_view,\n",
    "                height = fig_height, width = fig_width, autosize=False,\n",
    "                title=f\"END TO END AUTONOMOUS DRIVING {self.model_name}\", title_x=0.5, title_y=0.95,\n",
    "                margin=dict(r=0, b=0, l=0, t=0))\n",
    "        for row in range(2,4):\n",
    "            for col in range(1,3):\n",
    "                self.fig.update_xaxes(showticklabels=False, visible=False, row=row, col=col)\n",
    "                self.fig.update_yaxes(showticklabels=False, visible=False, row=row, col=col)\n",
    "        \n",
    "        # set export image option\n",
    "        self.fig.to_image(format=\"png\", engine=\"kaleido\")\n",
    "        self.pred_color = pred_box_color\n",
    "        self.waypoints_color = waypoints_color\n",
    "        self.box2d_color = bbox_2d_color\n",
    "\n",
    "    def clear_figure_data(self):\n",
    "        self.fig.data = []\n",
    "    \n",
    "    def get_bbox_colors(self, bbox_corners):\n",
    "        return [self.pred_color] * bbox_corners.shape[0] if bbox_corners is not None else None\n",
    "        \n",
    "    def plot_waypoints(self, waypoints):\n",
    "        return go.Mesh3d(x=waypoints[:,0], y=waypoints[:,1], z=waypoints[:,2], \n",
    "                         opacity=0.4, color=self.waypoints_color, \n",
    "                         hoverinfo='skip',showlegend=False)\n",
    "\n",
    "    def add_lidar_plots(self, points, waypoints, pred_corners=None):\n",
    "        lidar_3d_plots = get_lidar3d_plots(points, pc_kwargs=dict(colorscale='viridis', marker_size=0.9),\n",
    "                                   pred_box_corners = pred_corners, \n",
    "                                   pred_box_colors = self.get_bbox_colors(pred_corners))\n",
    "        lidar_3d_plots.append(self.plot_waypoints(waypoints))\n",
    "        for trace in lidar_3d_plots:\n",
    "            self.fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "    def add_image_plots(self, rgb_image, depth_image, lidar_data, pred_corners_2d):\n",
    "        self.fig.add_trace(get_image2d_plots(rgb_image), row=2, col=1)\n",
    "        \n",
    "        # repeating depth image 3 times to get standard channel\n",
    "        depth_image = np.tile(depth_image[:, :, None], (1,1,3))\n",
    "        depth_image = cv2.applyColorMap(np.uint8(255 * depth_image), cv2.COLORMAP_JET)\n",
    "        self.fig.add_trace(get_image2d_plots(depth_image), row=3, col=1)\n",
    "        \n",
    "        # BEV lidar image with bounding boxes\n",
    "        self.fig.add_trace(get_image2d_plots(lidar_data), row=2, col=2)\n",
    "        box_colors = [self.box2d_color] * len(pred_corners_2d)\n",
    "        for i, obj_i in enumerate(pred_corners_2d):\n",
    "            obj_plots = plot_box_corners2d(obj_i, color = box_colors[i])\n",
    "            for plot in obj_plots:\n",
    "                self.fig.add_trace(plot, row=2, col=2)\n",
    "        \n",
    "    def visualize_predictions(self, points, waypoints, pred_corners_3d, \n",
    "                              rgb_image, depth_image, lidar_data, pred_corners_2d):\n",
    "        # clear previous data and plot lidar, image data\n",
    "        self.clear_figure_data()\n",
    "        self.add_lidar_plots(points=points, waypoints=waypoints, pred_corners=pred_corners_3d)\n",
    "        self.add_image_plots(rgb_image, depth_image, lidar_data, pred_corners_2d)\n",
    "    \n",
    "    def show_figure(self):\n",
    "        self.fig.show()\n",
    "        \n",
    "    def save_to_png(self, output_path):\n",
    "        self.fig.write_image(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f45c6",
   "metadata": {
    "papermill": {
     "duration": 0.01484,
     "end_time": "2024-08-25T17:55:42.151068",
     "exception": false,
     "start_time": "2024-08-25T17:55:42.136228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "338526b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:42.182883Z",
     "iopub.status.busy": "2024-08-25T17:55:42.182511Z",
     "iopub.status.idle": "2024-08-25T17:55:42.196786Z",
     "shell.execute_reply": "2024-08-25T17:55:42.195960Z"
    },
    "papermill": {
     "duration": 0.032873,
     "end_time": "2024-08-25T17:55:42.199073",
     "exception": false,
     "start_time": "2024-08-25T17:55:42.166200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Eigen CAM imports\n",
    "from e2e_cam import EigenCAM, PlannerTarget, show_cam_on_image\n",
    "\n",
    "# which layers should be focus on\n",
    "target_layers = [model._model.image_encoder.features.layer4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a934472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:42.231681Z",
     "iopub.status.busy": "2024-08-25T17:55:42.230938Z",
     "iopub.status.idle": "2024-08-25T17:55:42.244597Z",
     "shell.execute_reply": "2024-08-25T17:55:42.243579Z"
    },
    "papermill": {
     "duration": 0.032346,
     "end_time": "2024-08-25T17:55:42.246837",
     "exception": false,
     "start_time": "2024-08-25T17:55:42.214491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import boxes_to_corners_3d\n",
    "\n",
    "def write_model_predictions(dataloader_demo, output_dir):\n",
    "    frameIdx = 0\n",
    "        \n",
    "    with EigenCAM(model=model, target_layers=target_layers) as cam:    \n",
    "        for data in tqdm(dataloader_demo):\n",
    "            # load data to device, according to type\n",
    "            for k in ['rgb', 'depth', 'lidar', 'label', 'ego_waypoint', \\\n",
    "                      'target_point', 'target_point_image', 'speed']:\n",
    "                data[k] = data[k].to(device, torch.float32)\n",
    "            for k in ['semantic', 'bev']:\n",
    "                data[k] = data[k].to(device, torch.long)\n",
    "\n",
    "            # get model predictions\n",
    "#             targets = [PlannerTarget(data['ego_waypoint'])]            \n",
    "            grayscale_cam = cam(input_data=data, targets=[], key=['rgb'])\n",
    "            outputs = cam.outputs[1]\n",
    "\n",
    "            # iterate through each sample in batch \n",
    "            bs = data['rgb'].shape[0]\n",
    "            for i in range(bs):\n",
    "                # input data\n",
    "                rgb_image = data['rgb'][i].permute(1, 2, 0).detach().cpu().numpy()\n",
    "                tgt_waypoints = data['ego_waypoint'][i].detach().cpu().numpy()\n",
    "                lidar_pc = data['raw_lidar'][i].detach().cpu().numpy()\n",
    "                num_points = data['num_raw_lidar_points'].detach().cpu().numpy()[i]\n",
    "                lidar_pc = lidar_pc[:num_points, :]\n",
    "\n",
    "                # bev lidar image\n",
    "                lidar_data = data['lidar'][i].detach().cpu().numpy().transpose(1,2,0)\n",
    "                lidar_data = (lidar_data * 255).astype(np.uint8)\n",
    "\n",
    "                # MODEL PREDICTIONS\n",
    "                pred_waypoints = outputs['pred_wp'][i]\n",
    "                pred_waypoints[:,1] *= -1\n",
    "                pred_lanepoints = generate_lane_points(pred_waypoints)\n",
    "\n",
    "                ## AUXILLARY TASK PREDICTIONS\n",
    "\n",
    "                ## bounding boxes\n",
    "                pred_boxes = outputs['detections'][i]\n",
    "                pred_3d_boxes = convert_to_3d_bboxes(pred_boxes)\n",
    "                pred_corners_3d = boxes_to_corners_3d(pred_3d_boxes)\n",
    "\n",
    "                # project to bev image space\n",
    "                pred_corners_2d = [get_rotated_bbox(bbox)[:, :2] for bbox in pred_boxes]\n",
    "\n",
    "                ## depth information\n",
    "                pred_depth = (outputs['pred_depth'][i] * 255).astype(np.uint8)\n",
    "                pred_bev = outputs['pred_bev'][i].argmax(axis=0).astype(np.uint8)\n",
    "                \n",
    "                # class activation maps\n",
    "                rgb_image = rgb_image.astype(np.uint8) \n",
    "#                 / 255.0\n",
    "#                 cam_image = show_cam_on_image(rgb_image, grayscale_cam[0][i, 0], use_rgb=True)\n",
    "                    \n",
    "                # plot all data\n",
    "                visualizer.visualize_predictions(lidar_pc, pred_lanepoints, pred_corners_3d,\n",
    "                                                 rgb_image, pred_depth, lidar_data, \n",
    "                                                 pred_corners_2d = pred_corners_2d)\n",
    "                # save figure\n",
    "                visualizer.save_to_png(os.path.join(output_dir, f\"Frame{frameIdx}.png\"))\n",
    "                frameIdx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364ebda8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:42.279194Z",
     "iopub.status.busy": "2024-08-25T17:55:42.278298Z",
     "iopub.status.idle": "2024-08-25T17:55:42.308776Z",
     "shell.execute_reply": "2024-08-25T17:55:42.307292Z"
    },
    "papermill": {
     "duration": 0.049145,
     "end_time": "2024-08-25T17:55:42.311137",
     "exception": false,
     "start_time": "2024-08-25T17:55:42.261992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "def convert_images_to_video(images_dir, output_video_path, fps : int = 8):\n",
    "    input_images = natsorted([x for x in os.listdir(images_dir) if x.endswith('png')])\n",
    "    input_images = [os.path.join(images_dir, x) for x in input_images]\n",
    "    \n",
    "    if(len(input_images) > 0):\n",
    "        sample_image = cv2.imread(input_images[0])\n",
    "        height, width, _ = sample_image.shape\n",
    "        \n",
    "        # handles for input output videos\n",
    "        output_handle = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "\n",
    "        # create progress bar\n",
    "        num_frames = int(len(input_images))\n",
    "        pbar = tqdm(total = num_frames, position=0, leave=True)\n",
    "\n",
    "        for i in tqdm(range(num_frames), position=0, leave=True):\n",
    "            frame = cv2.imread(input_images[i])\n",
    "            output_handle.write(frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "        # release the output video handler\n",
    "        output_handle.release()\n",
    "                \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "560b1019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:42.343246Z",
     "iopub.status.busy": "2024-08-25T17:55:42.342509Z",
     "iopub.status.idle": "2024-08-25T17:55:42.348540Z",
     "shell.execute_reply": "2024-08-25T17:55:42.347095Z"
    },
    "papermill": {
     "duration": 0.024291,
     "end_time": "2024-08-25T17:55:42.350605",
     "exception": false,
     "start_time": "2024-08-25T17:55:42.326314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_route_name(folder):\n",
    "    contains_routes = [x for x in folder.split('_') if 'route' in x and len(x)>=6]\n",
    "    if 'routes' in contains_routes:\n",
    "        contains_routes.remove('routes')\n",
    "    return contains_routes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f41d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T17:55:42.439790Z",
     "iopub.status.busy": "2024-08-25T17:55:42.439024Z",
     "iopub.status.idle": "2024-08-25T21:11:37.034311Z",
     "shell.execute_reply": "2024-08-25T21:11:37.033051Z"
    },
    "papermill": {
     "duration": 11754.671605,
     "end_time": "2024-08-25T21:11:37.037376",
     "exception": false,
     "start_time": "2024-08-25T17:55:42.365771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 49.28it/s]\n",
      "There are 86 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/mmdet/models/utils/gaussian_target.py:227: UserWarning:\n",
      "\n",
      "__floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/mmdet/models/utils/gaussian_target.py:229: UserWarning:\n",
      "\n",
      "__floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "\n",
      "100%|██████████| 43/43 [06:46<00:00,  9.45s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 86/86 [00:01<00:00, 52.71it/s]\n",
      "100%|██████████| 86/86 [00:01<00:00, 52.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route0 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 55.40it/s]\n",
      "There are 66 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [05:17<00:00,  9.61s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 66/66 [00:01<00:00, 50.48it/s]\n",
      "100%|██████████| 66/66 [00:01<00:00, 50.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route10 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 44.43it/s]\n",
      "There are 67 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [05:37<00:00,  9.93s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 67/67 [00:01<00:00, 52.63it/s]\n",
      "100%|██████████| 67/67 [00:01<00:00, 52.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route11 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 65.94it/s]\n",
      "There are 44 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [03:37<00:00,  9.90s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 44/44 [00:00<00:00, 51.04it/s]\n",
      "100%|██████████| 44/44 [00:00<00:00, 50.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route12 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.33it/s]\n",
      "There are 56 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [04:56<00:00, 10.60s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 56/56 [00:01<00:00, 41.49it/s]\n",
      "100%|██████████| 56/56 [00:01<00:00, 41.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route13 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.79it/s]\n",
      "There are 73 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [06:20<00:00, 10.29s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 73/73 [00:01<00:00, 52.91it/s]\n",
      "100%|██████████| 73/73 [00:01<00:00, 52.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route14 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 49.75it/s]\n",
      "There are 63 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [05:30<00:00, 10.34s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 63/63 [00:01<00:00, 54.07it/s]\n",
      "100%|██████████| 63/63 [00:01<00:00, 53.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route15 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 60.44it/s]\n",
      "There are 43 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [03:49<00:00, 10.44s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.82it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route16 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "There are 80 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [07:22<00:00, 11.07s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 80/80 [00:02<00:00, 37.39it/s]\n",
      "100%|██████████| 80/80 [00:02<00:00, 37.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route17 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 49.21it/s]\n",
      "There are 41 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [03:40<00:00, 10.51s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 41/41 [00:00<00:00, 54.66it/s]\n",
      "100%|██████████| 41/41 [00:00<00:00, 54.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route18 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 39.26it/s]\n",
      "There are 40 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:36<00:00, 10.80s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 40/40 [00:01<00:00, 32.22it/s]\n",
      "100%|██████████| 40/40 [00:01<00:00, 32.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route19 completed ...\n",
      "100%|██████████| 11/11 [00:00<00:00, 53.92it/s]\n",
      "There are 624 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [1:04:45<00:00, 12.45s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 624/624 [00:18<00:00, 34.02it/s]\n",
      "100%|██████████| 624/624 [00:18<00:00, 34.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route1 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 65.46it/s]\n",
      "There are 42 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [04:49<00:00, 13.80s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 42/42 [00:01<00:00, 34.20it/s]\n",
      "100%|██████████| 42/42 [00:01<00:00, 33.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route20 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 50.75it/s]\n",
      "There are 43 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [04:55<00:00, 13.43s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.96it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 51.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route21 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.39it/s]\n",
      "There are 36 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [03:27<00:00, 11.51s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 36/36 [00:00<00:00, 53.26it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 53.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route22 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 64.44it/s]\n",
      "There are 32 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:25<00:00,  9.07s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 32/32 [00:00<00:00, 53.32it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 53.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route23 completed ...\n",
      "100%|██████████| 5/5 [00:00<00:00, 95.36it/s]\n",
      "There are 211 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [16:49<00:00,  9.52s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 211/211 [00:03<00:00, 53.39it/s]\n",
      "100%|██████████| 211/211 [00:03<00:00, 53.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route2 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 29.29it/s]\n",
      "There are 68 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [05:32<00:00,  9.78s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 68/68 [00:01<00:00, 45.02it/s]\n",
      "100%|██████████| 68/68 [00:01<00:00, 44.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route3 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.95it/s]\n",
      "There are 60 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [05:04<00:00, 10.14s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 60/60 [00:01<00:00, 43.28it/s]\n",
      "100%|██████████| 60/60 [00:01<00:00, 43.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route4 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.18it/s]\n",
      "There are 129 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [11:23<00:00, 10.51s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 129/129 [00:02<00:00, 53.44it/s]\n",
      "100%|██████████| 129/129 [00:02<00:00, 53.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route5 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 49.46it/s]\n",
      "There are 43 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [03:56<00:00, 10.74s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 43/43 [00:00<00:00, 50.25it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 50.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route6 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.70it/s]\n",
      "There are 41 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [03:37<00:00, 10.36s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 41/41 [00:00<00:00, 51.18it/s]\n",
      "100%|██████████| 41/41 [00:00<00:00, 50.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route7 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 37.78it/s]\n",
      "There are 66 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [06:06<00:00, 11.09s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 66/66 [00:01<00:00, 33.46it/s]\n",
      "100%|██████████| 66/66 [00:01<00:00, 33.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route8 completed ...\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.96it/s]\n",
      "There are 56 samples in Demo dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [05:30<00:00, 11.79s/it]\n",
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "100%|██████████| 56/56 [00:01<00:00, 33.08it/s]\n",
      "100%|██████████| 56/56 [00:01<00:00, 32.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Town10HD_Scenario10 route9 completed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "visualizer = Visualizer(model_name='TRANSFUSER')\n",
    "\n",
    "root_dir = '/kaggle/input/carla-e2e-driving-data-2'\n",
    "scenarios = sorted(os.listdir(root_dir))\n",
    "\n",
    "for scenario in scenarios:\n",
    "    scenario_dir = os.path.join(root_dir, scenario)\n",
    "    routes = sorted(os.listdir(scenario_dir))\n",
    "    \n",
    "    for route in routes:\n",
    "        route_name = get_route_name(route)\n",
    "        route_dir = os.path.join(scenario_dir, route)\n",
    "        demo_set = CARLA_Data(root=scenario_dir, config=config, routeKey=route_name, load_raw_lidar=True)\n",
    "        print(f\"There are {len(demo_set)} samples in Demo dataset\")\n",
    "        dataloader_demo = DataLoader(demo_set, shuffle=False, batch_size=2, num_workers=4)\n",
    "        \n",
    "        # create output directory for scenario\n",
    "        output_dir = os.path.join(os.getcwd(), *[scenario, route_name])\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        \n",
    "        write_model_predictions(dataloader_demo, output_dir)\n",
    "        convert_images_to_video(output_dir, f'{scenario}_{route_name}_{model.pred_len}pts_{FPS}fps.mp4', fps=FPS)\n",
    "        print(f\"{scenario} {route_name} completed ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3184bb4",
   "metadata": {
    "papermill": {
     "duration": 0.23687,
     "end_time": "2024-08-25T21:11:37.513235",
     "exception": false,
     "start_time": "2024-08-25T21:11:37.276365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4922400,
     "sourceId": 8287377,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4849261,
     "sourceId": 9198582,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5559861,
     "sourceId": 9196580,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11880.32339,
   "end_time": "2024-08-25T21:11:40.513867",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-25T17:53:40.190477",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "403bc10c5c2a48f5b601bac6e6dc5633": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "458b934130764270a39d43f7f87a254e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c1387917bc234cb7a260ae32dcbbce73",
        "IPY_MODEL_c1205b8cffce48ee910e651c05b9601d",
        "IPY_MODEL_8041922f56d947dea38fda06325a2724"
       ],
       "layout": "IPY_MODEL_ad95996c9b184faba30d080a7a9a1645"
      }
     },
     "622c2d63e12f43848d54850090a242bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8041922f56d947dea38fda06325a2724": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ccf32315286b4391b95ff2df4d9adbdc",
       "placeholder": "​",
       "style": "IPY_MODEL_403bc10c5c2a48f5b601bac6e6dc5633",
       "value": " 78.1M/78.1M [00:05&lt;00:00, 18.0MB/s]"
      }
     },
     "90d74def023644eaa10f05dd7d843451": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad95996c9b184faba30d080a7a9a1645": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b306b8e593b84929bccaaca0ef5aea40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c1205b8cffce48ee910e651c05b9601d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_90d74def023644eaa10f05dd7d843451",
       "max": 78055620.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_622c2d63e12f43848d54850090a242bc",
       "value": 78055620.0
      }
     },
     "c1387917bc234cb7a260ae32dcbbce73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f26b40af700b4ee2879554a6ee7810f1",
       "placeholder": "​",
       "style": "IPY_MODEL_b306b8e593b84929bccaaca0ef5aea40",
       "value": "model.safetensors: 100%"
      }
     },
     "ccf32315286b4391b95ff2df4d9adbdc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f26b40af700b4ee2879554a6ee7810f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
