{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8287109,"sourceType":"datasetVersion","datasetId":4849257},{"sourceId":8287377,"sourceType":"datasetVersion","datasetId":4922400},{"sourceId":8956233,"sourceType":"datasetVersion","datasetId":4849261}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==1.11.0+cpu torchvision==0.12.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\n!pip install mmcv-full==1.5.3 -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.11/index.html\n!pip install mmdet==2.25.0 -f https://download.openmmlab.com/mmdet/dist/cpu/torch1.11/index.html\n!pip install kaleido","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:07:08.172168Z","iopub.execute_input":"2024-07-22T02:07:08.172566Z","iopub.status.idle":"2024-07-22T02:08:10.224157Z","shell.execute_reply.started":"2024-07-22T02:07:08.172530Z","shell.execute_reply":"2024-07-22T02:08:10.222476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport sys\nimport random\nimport scipy as sp\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nnp.set_printoptions(suppress=True, precision=5)\nsys.path.append('/kaggle/input/transfuser-e2e-scripts')\n\n# dl imports\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:10.226308Z","iopub.execute_input":"2024-07-22T02:08:10.226806Z","iopub.status.idle":"2024-07-22T02:08:10.940567Z","shell.execute_reply.started":"2024-07-22T02:08:10.226757Z","shell.execute_reply":"2024-07-22T02:08:10.939232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CARLA dataset","metadata":{}},{"cell_type":"code","source":"from config import GlobalConfig\nfrom data import CARLA_Data\n\nconfig = GlobalConfig()\nconfig.pred_len = 7\n\nFPS = 8","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:10.942089Z","iopub.execute_input":"2024-07-22T02:08:10.942632Z","iopub.status.idle":"2024-07-22T02:08:11.149472Z","shell.execute_reply.started":"2024-07-22T02:08:10.942592Z","shell.execute_reply":"2024-07-22T02:08:11.148180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create pytorch style dataloaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:11.153707Z","iopub.execute_input":"2024-07-22T02:08:11.154428Z","iopub.status.idle":"2024-07-22T02:08:11.160734Z","shell.execute_reply.started":"2024-07-22T02:08:11.154370Z","shell.execute_reply":"2024-07-22T02:08:11.159257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load pretrained model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n\nfrom model import LidarCenterNet\nmodel = LidarCenterNet(config, device, config.backbone, image_architecture='regnety_032', \n                           lidar_architecture='regnety_032')\nmodel.to(device);\nmodel.config.debug = True\n\nmodel.eval();\ncheckpt = torch.load('/kaggle/input/carla-transfuser-regnet032/transfuser_regnet032_seed1_39.pth', map_location=device)\nmodel.load_state_dict(checkpt)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:11.162270Z","iopub.execute_input":"2024-07-22T02:08:11.162649Z","iopub.status.idle":"2024-07-22T02:08:20.762174Z","shell.execute_reply.started":"2024-07-22T02:08:11.162614Z","shell.execute_reply":"2024-07-22T02:08:20.761112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"from utils import VEHICLE_TO_LIDAR_FWD, LIDAR_HEIGHT\n\ndef generate_lane_points(waypoints, lane_width = 1.0):\n    # input waypoints are 2d coordinates of centerline (N,2)\n    # this function generates left and right lane corners\n    # by subtracting and adding half lane width. We convert\n    # to 3D Lidar coordinates by placing points at ground level\n\n    n_points = waypoints.shape[0]\n    lane_points = np.zeros((n_points * 2 , 3))\n    \n    # vehicle to lidar frame\n    lane_points[:n_points, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n    lane_points[n_points:, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n    \n    # left and right lanes\n    lane_points[:n_points,1] = waypoints[:,1] - (lane_width * 0.5)\n    lane_points[n_points:,1] = waypoints[:,1] + (lane_width * 0.5)\n    \n    # fixed height\n    lane_points[:,2] = -LIDAR_HEIGHT\n    return lane_points","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:20.764155Z","iopub.execute_input":"2024-07-22T02:08:20.764665Z","iopub.status.idle":"2024-07-22T02:08:20.773574Z","shell.execute_reply.started":"2024-07-22T02:08:20.764619Z","shell.execute_reply":"2024-07-22T02:08:20.772278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bev_to_lidar = np.array([\n            [0, -(1/8.0), 32],\n            [-(1/8.0), 0, 16],\n            [0 , 0, 1]\n])\n\n\ndef convert_to_3d_bboxes(boxes_2d):\n    n_boxes = boxes_2d.shape[0]\n    bbox_3d = np.zeros((n_boxes, 7))\n\n    # xy position from bev pixels to metres\n    homogenous_coordinates = np.hstack([boxes_2d[:, :2], np.ones((n_boxes, 1))])\n    bbox_3d[:, :2] = (bev_to_lidar @ homogenous_coordinates.T).T[:, :2]\n    bbox_3d[:, 3] = boxes_2d[:, 3] / 8  # length \n    bbox_3d[:, 4] = boxes_2d[:, 2] / 8  # width\n    bbox_3d[:, 6] = boxes_2d[:, 4]      # yaw\n\n    # hardcoding z values\n    bbox_3d[:, 2] = -1.25\n    bbox_3d[:, 5] = 2.5\n    return bbox_3d","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:20.775160Z","iopub.execute_input":"2024-07-22T02:08:20.775590Z","iopub.status.idle":"2024-07-22T02:08:20.788545Z","shell.execute_reply.started":"2024-07-22T02:08:20.775556Z","shell.execute_reply":"2024-07-22T02:08:20.787271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_rotated_bbox(bbox):\n    x, y, w, h, yaw, _, _  =  bbox\n\n    bbox = np.array([[h,   w, 1],\n                     [h,  -w, 1],\n                     [-h, -w, 1],\n                     [-h,  w, 1],\n                ])\n    \n    # The height and width of the bounding box value was changed by this factor \n    # during data collection. Fix that for future datasets and remove    \n    bbox[:, :2] /= 2\n    bbox[:, :2] = bbox[:, [1, 0]]\n\n    c, s = np.cos(yaw), np.sin(yaw)\n    # use y x because coordinate is changed\n    r1_to_world = np.array([[c, -s, x], [s, c, y], [0, 0, 1]])\n    bbox = r1_to_world @ bbox.T\n    bbox = bbox.T\n    bbox = np.clip(bbox, 0, 256)\n    return bbox\n\ndef get_scatter_plot(x,y, mode='lines', marker_size=2, color=None, **kwargs):\n    return go.Scatter(x=x, y=y, mode=mode, hoverinfo='skip',showlegend=False, \n                        marker = dict(size=marker_size, color=color), **kwargs)\n\ndef plot_box_corners2d(box2d, color,**kwargs):\n    return [\n        get_scatter_plot([box2d[0,0], box2d[1,0]], [box2d[0,1], box2d[1,1]], color=color, **kwargs),\n        get_scatter_plot([box2d[1,0], box2d[2,0]], [box2d[1,1], box2d[2,1]], color=color, **kwargs),\n        get_scatter_plot([box2d[2,0], box2d[3,0]], [box2d[2,1], box2d[3,1]], color=color, **kwargs),\n        get_scatter_plot([box2d[3,0], box2d[0,0]], [box2d[3,1], box2d[0,1]], color=color, **kwargs),\n    ]","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:20.790098Z","iopub.execute_input":"2024-07-22T02:08:20.790515Z","iopub.status.idle":"2024-07-22T02:08:20.807287Z","shell.execute_reply.started":"2024-07-22T02:08:20.790475Z","shell.execute_reply":"2024-07-22T02:08:20.806074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization class","metadata":{}},{"cell_type":"code","source":"PCD_CAM_VIEW = dict(\n            up=dict(x=0, y=0, z=1),\n            eye=dict(x=-0.9, y=0, z=0.2)\n    )\n\nPCD_SCENE=dict(\n        xaxis=dict(visible=False),\n        yaxis=dict(visible=False),\n        zaxis=dict(visible=False,),\n        aspectmode='manual',\n        aspectratio=dict(x=1, y=1, z=0.1),\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:20.808700Z","iopub.execute_input":"2024-07-22T02:08:20.809100Z","iopub.status.idle":"2024-07-22T02:08:20.832892Z","shell.execute_reply.started":"2024-07-22T02:08:20.809068Z","shell.execute_reply":"2024-07-22T02:08:20.831548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from utils import get_lidar3d_plots, get_image2d_plots\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nclass Visualizer:\n    def __init__(self, model_name, fig_width=1000, fig_height=800, pred_box_color='orange', \n                 waypoints_color = 'red', bbox_2d_color = 'cyan', scene=PCD_SCENE, cam_view=PCD_CAM_VIEW):\n        self.model_name = model_name\n\n        # Create a 2x3 figure, top row for point cloud data\n        # bottom row for rgb image data\n        self.fig = make_subplots(rows=3, cols=2,\n                                 specs=[[{\"type\": \"scatter3d\", \"colspan\": 2}, None], \n                                        [{}, {\"rowspan\": 2}],\n                                        [{}, None]], \n                                row_heights=[0.6, 0.2, 0.2], horizontal_spacing=0.0, vertical_spacing = 0.0)\n        \n        self.fig.update_layout(template=\"plotly_dark\", scene=scene, scene_camera = cam_view,\n                height = fig_height, width = fig_width, autosize=False,\n                title=f\"END TO END AUTONOMOUS DRIVING {self.model_name}\", title_x=0.5, title_y=0.95,\n                margin=dict(r=0, b=0, l=0, t=0))\n        for row in range(2,4):\n            for col in range(1,3):\n                self.fig.update_xaxes(showticklabels=False, visible=False, row=row, col=col)\n                self.fig.update_yaxes(showticklabels=False, visible=False, row=row, col=col)\n        \n        # set export image option\n        self.fig.to_image(format=\"png\", engine=\"kaleido\")\n        self.pred_color = pred_box_color\n        self.waypoints_color = waypoints_color\n        self.box2d_color = bbox_2d_color\n\n    def clear_figure_data(self):\n        self.fig.data = []\n    \n    def get_bbox_colors(self, bbox_corners):\n        return [self.pred_color] * bbox_corners.shape[0] if bbox_corners is not None else None\n        \n    def plot_waypoints(self, waypoints):\n        return go.Mesh3d(x=waypoints[:,0], y=waypoints[:,1], z=waypoints[:,2], \n                         opacity=0.4, color=self.waypoints_color, \n                         hoverinfo='skip',showlegend=False)\n\n    def add_lidar_plots(self, points, waypoints, pred_corners=None):\n        lidar_3d_plots = get_lidar3d_plots(points, pc_kwargs=dict(colorscale='viridis', marker_size=0.9),\n                                   pred_box_corners = pred_corners, \n                                   pred_box_colors = self.get_bbox_colors(pred_corners))\n        lidar_3d_plots.append(self.plot_waypoints(waypoints))\n        for trace in lidar_3d_plots:\n            self.fig.add_trace(trace, row=1, col=1)\n\n    def add_image_plots(self, rgb_image, depth_image, lidar_data, pred_corners_2d):\n        self.fig.add_trace(get_image2d_plots(rgb_image), row=2, col=1)\n        \n        # repeating depth image 3 times to get standard channel\n        depth_image = np.tile(depth_image[:, :, None], (1,1,3))\n        self.fig.add_trace(get_image2d_plots(depth_image), row=3, col=1)\n        \n        # BEV lidar image with bounding boxes\n        self.fig.add_trace(get_image2d_plots(lidar_data), row=2, col=2)\n        box_colors = [self.box2d_color] * len(pred_corners_2d)\n        for i, obj_i in enumerate(pred_corners_2d):\n            obj_plots = plot_box_corners2d(obj_i, color = box_colors[i])\n            for plot in obj_plots:\n                self.fig.add_trace(plot, row=2, col=2)\n        \n    def visualize_predictions(self, points, waypoints, pred_corners_3d, \n                              rgb_image, depth_image, lidar_data, pred_corners_2d):\n        # clear previous data and plot lidar, image data\n        self.clear_figure_data()\n        self.add_lidar_plots(points=points, waypoints=waypoints, pred_corners=pred_corners_3d)\n        self.add_image_plots(rgb_image, depth_image, lidar_data, pred_corners_2d)\n    \n    def show_figure(self):\n        self.fig.show()\n        \n    def save_to_png(self, output_path):\n        self.fig.write_image(output_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:20.834628Z","iopub.execute_input":"2024-07-22T02:08:20.835156Z","iopub.status.idle":"2024-07-22T02:08:20.874672Z","shell.execute_reply.started":"2024-07-22T02:08:20.835035Z","shell.execute_reply":"2024-07-22T02:08:20.873068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Demo video","metadata":{}},{"cell_type":"code","source":"import os\nfrom utils import boxes_to_corners_3d\n\ndef write_model_predictions(dataloader_demo, output_dir):\n    frameIdx = 0\n    for data in tqdm(dataloader_demo):\n\n        # load data to device, according to type\n        for k in ['rgb', 'depth', 'lidar', 'label', 'ego_waypoint', \\\n                  'target_point', 'target_point_image', 'speed']:\n            data[k] = data[k].to(device, torch.float32)\n        for k in ['semantic', 'bev']:\n            data[k] = data[k].to(device, torch.long)\n\n        # get model predictions\n        _, outputs = model(data)\n\n        # iterate through each sample in batch \n        bs = data['rgb'].shape[0]\n        for i in range(bs):\n            # input data\n            rgb_image = data['rgb'][i].permute(1, 2, 0).detach().cpu().numpy().astype(np.uint8)\n            tgt_waypoints = data['ego_waypoint'][i].detach().cpu().numpy()\n            lidar_pc = data['raw_lidar'][i].detach().cpu().numpy()\n            num_points = data['num_raw_lidar_points'].detach().cpu().numpy()[i]\n            lidar_pc = lidar_pc[:num_points, :]\n\n            # bev lidar image\n            lidar_data = data['lidar'][i].detach().cpu().numpy().transpose(1,2,0)\n            lidar_data = (lidar_data * 255).astype(np.uint8)\n\n            # MODEL PREDICTIONS\n            pred_waypoints = outputs['pred_wp'][i]\n            pred_lanepoints = generate_lane_points(pred_waypoints)\n\n            ## AUXILLARY TASK PREDICTIONS\n\n            ## bounding boxes\n            pred_boxes = outputs['detections'][i]\n            pred_3d_boxes = convert_to_3d_bboxes(pred_boxes)\n            pred_corners_3d = boxes_to_corners_3d(pred_3d_boxes)\n\n            # project to bev image space\n            pred_corners_2d = [get_rotated_bbox(bbox)[:, :2] for bbox in pred_boxes]\n\n            ## depth information\n            pred_depth = (outputs['pred_depth'][i] * 255).astype(np.uint8)\n            pred_bev = outputs['pred_bev'][i].argmax(axis=0).astype(np.uint8)\n\n            # plot all data\n            visualizer.visualize_predictions(lidar_pc, pred_lanepoints, pred_corners_3d,\n                                             rgb_image, pred_depth, lidar_data, \n                                             pred_corners_2d = pred_corners_2d)\n            # save figure\n            visualizer.save_to_png(os.path.join(output_dir, f\"Frame{frameIdx}.png\"))\n            frameIdx +=1","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:20.876551Z","iopub.execute_input":"2024-07-22T02:08:20.877082Z","iopub.status.idle":"2024-07-22T02:08:20.895911Z","shell.execute_reply.started":"2024-07-22T02:08:20.877034Z","shell.execute_reply":"2024-07-22T02:08:20.894319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_images_to_video(images_dir, output_video_path, fps : int = 8):\n    input_images = [os.path.join(images_dir, *[x]) for x in sorted(os.listdir(images_dir)) if x.endswith('png')]\n    \n    if(len(input_images) > 0):\n        sample_image = cv2.imread(input_images[0])\n        height, width, _ = sample_image.shape\n        \n        # handles for input output videos\n        output_handle = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n\n        # create progress bar\n        num_frames = int(len(input_images))\n        pbar = tqdm(total = num_frames, position=0, leave=True)\n\n        for i in tqdm(range(num_frames), position=0, leave=True):\n            frame = cv2.imread(input_images[i])\n            output_handle.write(frame)\n            pbar.update(1)\n\n        # release the output video handler\n        output_handle.release()\n                \n    else:\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:20.897765Z","iopub.execute_input":"2024-07-22T02:08:20.898161Z","iopub.status.idle":"2024-07-22T02:08:20.914254Z","shell.execute_reply.started":"2024-07-22T02:08:20.898125Z","shell.execute_reply":"2024-07-22T02:08:20.913127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_route_name(folder):\n    contains_routes = [x for x in folder.split('_') if 'route' in x and len(x) ==6]\n    if 'routes' in contains_routes:\n        contains_routes.remove('routes')\n    return contains_routes[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:20.918190Z","iopub.execute_input":"2024-07-22T02:08:20.919406Z","iopub.status.idle":"2024-07-22T02:08:20.931591Z","shell.execute_reply.started":"2024-07-22T02:08:20.919366Z","shell.execute_reply":"2024-07-22T02:08:20.930288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib agg\nvisualizer = Visualizer(model_name='TRANSFUSER')\n\nroot_dir = '/kaggle/input/carla-e2e-data/demo/'\nscenarios = sorted(os.listdir(root_dir))\nfor scenario in scenarios:\n    scenario_dir = os.path.join(root_dir, scenario)\n    routes = sorted(os.listdir(scenario_dir))\n    \n    for route in routes:\n        route_name = get_route_name(route)\n        route_dir = os.path.join(scenario_dir, route)\n        demo_set = CARLA_Data(root=scenario_dir, config=config, routeKey=route_name, load_raw_lidar=True)\n        print(f\"There are {len(demo_set)} samples in Demo dataset\")\n        dataloader_demo = DataLoader(demo_set, shuffle=False, batch_size=2, num_workers=4)\n        \n        # create output directory for scenario\n        output_dir = os.path.join(os.getcwd(), *[scenario, route_name])\n        os.makedirs(output_dir, exist_ok = True)\n        \n        write_model_predictions(dataloader_demo, output_dir)\n        convert_images_to_video(output_dir, f'{scenario}_{route_name}_{model.pred_len}pts_{FPS}fps.mp4', fps=FPS)\n        print(f\"{scenario} {route_name} completed ...\")","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:08:20.932985Z","iopub.execute_input":"2024-07-22T02:08:20.933480Z","iopub.status.idle":"2024-07-22T02:14:50.356537Z","shell.execute_reply.started":"2024-07-22T02:08:20.933441Z","shell.execute_reply":"2024-07-22T02:14:50.354872Z"},"trusted":true},"execution_count":null,"outputs":[]}]}