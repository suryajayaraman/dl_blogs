{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8287109,"sourceType":"datasetVersion","datasetId":4849257},{"sourceId":8287377,"sourceType":"datasetVersion","datasetId":4922400},{"sourceId":8956233,"sourceType":"datasetVersion","datasetId":4849261}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==1.11.0+cpu torchvision==0.12.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\n!pip install mmcv-full==1.5.3 -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.11/index.html\n!pip install mmdet==2.25.0 -f https://download.openmmlab.com/mmdet/dist/cpu/torch1.11/index.html\n!pip install kaleido","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:27:44.364485Z","iopub.execute_input":"2024-07-22T08:27:44.365024Z","iopub.status.idle":"2024-07-22T08:28:39.193326Z","shell.execute_reply.started":"2024-07-22T08:27:44.364972Z","shell.execute_reply":"2024-07-22T08:28:39.190799Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\nRequirement already satisfied: torch==1.11.0+cpu in /opt/conda/lib/python3.10/site-packages (1.11.0+cpu)\nRequirement already satisfied: torchvision==0.12.0+cpu in /opt/conda/lib/python3.10/site-packages (0.12.0+cpu)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.11.0+cpu) (4.9.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0+cpu) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0+cpu) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.12.0+cpu) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.12.0+cpu) (2024.2.2)\nLooking in links: https://download.openmmlab.com/mmcv/dist/cpu/torch1.11/index.html\nRequirement already satisfied: mmcv-full==1.5.3 in /opt/conda/lib/python3.10/site-packages (1.5.3)\nRequirement already satisfied: addict in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (21.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (9.5.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (6.0.1)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (0.40.2)\nRequirement already satisfied: opencv-python>=3 in /opt/conda/lib/python3.10/site-packages (from mmcv-full==1.5.3) (4.9.0.80)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->mmcv-full==1.5.3) (3.1.1)\nRequirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->mmcv-full==1.5.3) (6.11.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmcv-full==1.5.3) (4.2.0)\nRequirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmcv-full==1.5.3) (2.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->mmcv-full==1.5.3) (3.17.0)\nLooking in links: https://download.openmmlab.com/mmdet/dist/cpu/torch1.11/index.html\nRequirement already satisfied: mmdet==2.25.0 in /opt/conda/lib/python3.10/site-packages (2.25.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (1.26.4)\nRequirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (2.0.8)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (1.16.0)\nRequirement already satisfied: terminaltables in /opt/conda/lib/python3.10/site-packages (from mmdet==2.25.0) (3.1.10)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmdet==2.25.0) (2.9.0.post0)\nRequirement already satisfied: kaleido in /opt/conda/lib/python3.10/site-packages (0.2.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport cv2\nimport sys\nimport random\nimport scipy as sp\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nnp.set_printoptions(suppress=True, precision=5)\nsys.path.append('/kaggle/input/transfuser-e2e-scripts')\n\n# dl imports\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:39.201094Z","iopub.execute_input":"2024-07-22T08:28:39.202836Z","iopub.status.idle":"2024-07-22T08:28:39.888086Z","shell.execute_reply.started":"2024-07-22T08:28:39.202772Z","shell.execute_reply":"2024-07-22T08:28:39.886728Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## CARLA dataset","metadata":{}},{"cell_type":"code","source":"from config import GlobalConfig\nfrom data import CARLA_Data\n\nroot_dir = '/kaggle/input/carla-e2e-data/demo/scenario1/'\nconfig = GlobalConfig()\nconfig.pred_len = 7\ndemo_set = CARLA_Data(root=root_dir, config=config, routeKey='route0', load_raw_lidar=True)\nprint(f\"There are {len(demo_set)} samples in Demo dataset\")","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:39.889601Z","iopub.execute_input":"2024-07-22T08:28:39.890098Z","iopub.status.idle":"2024-07-22T08:28:40.191070Z","shell.execute_reply.started":"2024-07-22T08:28:39.890064Z","shell.execute_reply":"2024-07-22T08:28:40.189840Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"100%|██████████| 1/1 [00:00<00:00, 72.15it/s]\nThere are 95 samples in Demo dataset\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Create pytorch style dataloaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ndataloader_demo = DataLoader(demo_set, shuffle=False, batch_size=2, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:40.193809Z","iopub.execute_input":"2024-07-22T08:28:40.194288Z","iopub.status.idle":"2024-07-22T08:28:40.200662Z","shell.execute_reply.started":"2024-07-22T08:28:40.194254Z","shell.execute_reply":"2024-07-22T08:28:40.199395Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sample_data = next(iter(dataloader_demo))\nprint(f\"sample data is of type {type(sample_data)} and has following keys\")\n\nfor k,v in sample_data.items():\n    print(k, list(v.shape))\n    \ndel sample_data","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:40.202090Z","iopub.execute_input":"2024-07-22T08:28:40.202663Z","iopub.status.idle":"2024-07-22T08:28:41.356611Z","shell.execute_reply.started":"2024-07-22T08:28:40.202598Z","shell.execute_reply":"2024-07-22T08:28:41.354653Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"sample data is of type <class 'dict'> and has following keys\nrgb [2, 3, 160, 704]\nbev [2, 160, 160]\ndepth [2, 160, 704]\nsemantic [2, 160, 704]\nspeed [2]\nx_command [2]\ny_command [2]\ntarget_point [2, 2]\ntarget_point_image [2, 1, 256, 256]\nraw_lidar [2, 10000, 3]\nnum_raw_lidar_points [2]\nlidar [2, 2, 256, 256]\nlabel [2, 20, 7]\nego_waypoint [2, 7, 2]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load pretrained model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n\nfrom model import LidarCenterNet\nmodel = LidarCenterNet(config, device, config.backbone, image_architecture='regnety_032', \n                           lidar_architecture='regnety_032')\nmodel.to(device);\nmodel.config.debug = True\n\nmodel.eval();\ncheckpt = torch.load('/kaggle/input/carla-transfuser-regnet032/transfuser_regnet032_seed1_39.pth', map_location=device)\nmodel.load_state_dict(checkpt)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:41.358913Z","iopub.execute_input":"2024-07-22T08:28:41.359377Z","iopub.status.idle":"2024-07-22T08:28:50.801453Z","shell.execute_reply.started":"2024-07-22T08:28:41.359338Z","shell.execute_reply":"2024-07-22T08:28:50.800181Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"from utils import VEHICLE_TO_LIDAR_FWD, LIDAR_HEIGHT\n\ndef generate_lane_points(waypoints, lane_width = 1.0):\n    # input waypoints are 2d coordinates of centerline (N,2)\n    # this function generates left and right lane corners\n    # by subtracting and adding half lane width. We convert\n    # to 3D Lidar coordinates by placing points at ground level\n\n    n_points = waypoints.shape[0]\n    lane_points = np.zeros((n_points * 2 , 3))\n    \n    # vehicle to lidar frame\n    lane_points[:n_points, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n    lane_points[n_points:, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n    \n    # left and right lanes\n    lane_points[:n_points,1] = waypoints[:,1] - (lane_width * 0.5)\n    lane_points[n_points:,1] = waypoints[:,1] + (lane_width * 0.5)\n    \n    # fixed height\n    lane_points[:,2] = -LIDAR_HEIGHT\n    return lane_points","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:50.803067Z","iopub.execute_input":"2024-07-22T08:28:50.803476Z","iopub.status.idle":"2024-07-22T08:28:50.811581Z","shell.execute_reply.started":"2024-07-22T08:28:50.803442Z","shell.execute_reply":"2024-07-22T08:28:50.810158Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"bev_to_lidar = np.array([\n            [0, -(1/8.0), 32],\n            [-(1/8.0), 0, 16],\n            [0 , 0, 1]\n])\n\n\ndef convert_to_3d_bboxes(boxes_2d):\n    n_boxes = boxes_2d.shape[0]\n    bbox_3d = np.zeros((n_boxes, 7))\n\n    # xy position from bev pixels to metres\n    homogenous_coordinates = np.hstack([boxes_2d[:, :2], np.ones((n_boxes, 1))])\n    bbox_3d[:, :2] = (bev_to_lidar @ homogenous_coordinates.T).T[:, :2]\n    bbox_3d[:, 3] = boxes_2d[:, 3] / 8  # length \n    bbox_3d[:, 4] = boxes_2d[:, 2] / 8  # width\n    bbox_3d[:, 6] = -boxes_2d[:, 4]      # yaw\n\n    # hardcoding z values\n    bbox_3d[:, 2] = -1.25\n    bbox_3d[:, 5] = 2.5\n    return bbox_3d","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:50.813468Z","iopub.execute_input":"2024-07-22T08:28:50.814244Z","iopub.status.idle":"2024-07-22T08:28:50.825713Z","shell.execute_reply.started":"2024-07-22T08:28:50.814187Z","shell.execute_reply":"2024-07-22T08:28:50.824563Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_rotated_bbox(bbox):\n    x, y, w, h, yaw, _, _  =  bbox\n\n    bbox = np.array([[h,   w, 1],\n                     [h,  -w, 1],\n                     [-h, -w, 1],\n                     [-h,  w, 1],\n                ])\n    \n    # The height and width of the bounding box value was changed by this factor \n    # during data collection. Fix that for future datasets and remove    \n    bbox[:, :2] /= 2\n    bbox[:, :2] = bbox[:, [1, 0]]\n\n    c, s = np.cos(yaw), np.sin(yaw)\n    # use y x because coordinate is changed\n    r1_to_world = np.array([[c, -s, x], [s, c, y], [0, 0, 1]])\n    bbox = r1_to_world @ bbox.T\n    bbox = bbox.T\n    bbox = np.clip(bbox, 0, 256)\n    return bbox","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:50.827213Z","iopub.execute_input":"2024-07-22T08:28:50.827728Z","iopub.status.idle":"2024-07-22T08:28:50.844474Z","shell.execute_reply.started":"2024-07-22T08:28:50.827684Z","shell.execute_reply":"2024-07-22T08:28:50.843188Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_scatter_plot(x,y, mode='lines', marker_size=2, color=None, **kwargs):\n    return go.Scatter(x=x, y=y, mode=mode, hoverinfo='skip',showlegend=False, \n                        marker = dict(size=marker_size, color=color), **kwargs)\n\ndef plot_box_corners2d(box2d, color,**kwargs):\n    return [\n        get_scatter_plot([box2d[0,0], box2d[1,0]], [box2d[0,1], box2d[1,1]], color=color, **kwargs),\n        get_scatter_plot([box2d[1,0], box2d[2,0]], [box2d[1,1], box2d[2,1]], color=color, **kwargs),\n        get_scatter_plot([box2d[2,0], box2d[3,0]], [box2d[2,1], box2d[3,1]], color=color, **kwargs),\n        get_scatter_plot([box2d[3,0], box2d[0,0]], [box2d[3,1], box2d[0,1]], color=color, **kwargs),\n    ]","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:50.846084Z","iopub.execute_input":"2024-07-22T08:28:50.846655Z","iopub.status.idle":"2024-07-22T08:28:50.859316Z","shell.execute_reply.started":"2024-07-22T08:28:50.846593Z","shell.execute_reply":"2024-07-22T08:28:50.858066Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Visualization class","metadata":{}},{"cell_type":"code","source":"PCD_CAM_VIEW = dict(\n            up=dict(x=0, y=0, z=1),\n            eye=dict(x=-0.9, y=0, z=0.2)\n    )\n\nPCD_SCENE=dict(\n        xaxis=dict(visible=False),\n        yaxis=dict(visible=False),\n        zaxis=dict(visible=False,),\n        aspectmode='manual',\n        aspectratio=dict(x=1, y=1, z=0.1),\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:50.861030Z","iopub.execute_input":"2024-07-22T08:28:50.861510Z","iopub.status.idle":"2024-07-22T08:28:50.888006Z","shell.execute_reply.started":"2024-07-22T08:28:50.861467Z","shell.execute_reply":"2024-07-22T08:28:50.886708Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from utils import get_lidar3d_plots, get_image2d_plots\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nclass Visualizer:\n    def __init__(self, model_name, fig_width=1000, fig_height=800, pred_box_color='orange', \n                 waypoints_color = 'red', bbox_2d_color = 'cyan', scene=PCD_SCENE, cam_view=PCD_CAM_VIEW):\n        self.model_name = model_name\n\n        # Create a 2x3 figure, top row for point cloud data\n        # bottom row for rgb image data\n        self.fig = make_subplots(rows=3, cols=2,\n                                 specs=[[{\"type\": \"scatter3d\", \"colspan\": 2}, None], \n                                        [{}, {\"rowspan\": 2}],\n                                        [{}, None]], \n                                row_heights=[0.6, 0.2, 0.2], horizontal_spacing=0.0, vertical_spacing = 0.0)\n        \n        self.fig.update_layout(template=\"plotly_dark\", scene=scene, scene_camera = cam_view,\n                height = fig_height, width = fig_width, autosize=False,\n                title=f\"END TO END AUTONOMOUS DRIVING {self.model_name}\", title_x=0.5, title_y=0.95,\n                margin=dict(r=0, b=0, l=0, t=0))\n        for row in range(2,4):\n            for col in range(1,3):\n                self.fig.update_xaxes(showticklabels=False, visible=False, row=row, col=col)\n                self.fig.update_yaxes(showticklabels=False, visible=False, row=row, col=col)\n        \n        # set export image option\n        self.fig.to_image(format=\"png\", engine=\"kaleido\")\n        self.pred_color = pred_box_color\n        self.waypoints_color = waypoints_color\n        self.box2d_color = bbox_2d_color\n\n    def clear_figure_data(self):\n        self.fig.data = []\n    \n    def get_bbox_colors(self, bbox_corners):\n        return [self.pred_color] * bbox_corners.shape[0] if bbox_corners is not None else None\n        \n    def plot_waypoints(self, waypoints):\n        return go.Mesh3d(x=waypoints[:,0], y=waypoints[:,1], z=waypoints[:,2], \n                         opacity=0.4, color=self.waypoints_color, \n                         hoverinfo='skip',showlegend=False)\n\n    def add_lidar_plots(self, points, waypoints, pred_corners=None):\n        lidar_3d_plots = get_lidar3d_plots(points, pc_kwargs=dict(colorscale='viridis', marker_size=0.9),\n                                   pred_box_corners = pred_corners, \n                                   pred_box_colors = self.get_bbox_colors(pred_corners))\n        lidar_3d_plots.append(self.plot_waypoints(waypoints))\n        for trace in lidar_3d_plots:\n            self.fig.add_trace(trace, row=1, col=1)\n\n    def add_image_plots(self, rgb_image, depth_image, lidar_data, pred_corners_2d):\n        self.fig.add_trace(get_image2d_plots(rgb_image), row=2, col=1)\n        \n        # repeating depth image 3 times to get standard channel\n        depth_image = np.tile(depth_image[:, :, None], (1,1,3))\n        self.fig.add_trace(get_image2d_plots(depth_image), row=3, col=1)\n        \n        # BEV lidar image with bounding boxes\n        self.fig.add_trace(get_image2d_plots(lidar_data), row=2, col=2)\n        box_colors = [self.box2d_color] * len(pred_corners_2d)\n        for i, obj_i in enumerate(pred_corners_2d):\n            obj_plots = plot_box_corners2d(obj_i, color = box_colors[i])\n            for plot in obj_plots:\n                self.fig.add_trace(plot, row=2, col=2)\n        \n    def visualize_predictions(self, points, waypoints, pred_corners_3d, \n                              rgb_image, depth_image, lidar_data, pred_corners_2d):\n        # clear previous data and plot lidar, image data\n        self.clear_figure_data()\n        self.add_lidar_plots(points=points, waypoints=waypoints, pred_corners=pred_corners_3d)\n        self.add_image_plots(rgb_image, depth_image, lidar_data, pred_corners_2d)\n    \n    def show_figure(self):\n        self.fig.show()\n        \n    def save_to_png(self, output_path):\n        self.fig.write_image(output_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:50.889711Z","iopub.execute_input":"2024-07-22T08:28:50.890216Z","iopub.status.idle":"2024-07-22T08:28:50.923882Z","shell.execute_reply.started":"2024-07-22T08:28:50.890157Z","shell.execute_reply":"2024-07-22T08:28:50.922539Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Demo video","metadata":{}},{"cell_type":"code","source":"%matplotlib agg\nvisualizer = Visualizer(model_name='TRANSFUSER')\n\nfrom utils import boxes_to_corners_3d\n\nframeIdx = 0\nfor data in tqdm(dataloader_demo):\n\n    # load data to device, according to type\n    for k in ['rgb', 'depth', 'lidar', 'label', 'ego_waypoint', \\\n              'target_point', 'target_point_image', 'speed']:\n        data[k] = data[k].to(device, torch.float32)\n    for k in ['semantic', 'bev']:\n        data[k] = data[k].to(device, torch.long)\n    \n    # get model predictions\n    _, outputs = model(data)\n\n    # iterate through each sample in batch \n    bs = data['rgb'].shape[0]\n    for i in range(bs):\n        # input data\n        rgb_image = data['rgb'][i].permute(1, 2, 0).detach().cpu().numpy().astype(np.uint8)\n        tgt_waypoints = data['ego_waypoint'][i].detach().cpu().numpy()\n        lidar_pc = data['raw_lidar'][i].detach().cpu().numpy()\n        num_points = data['num_raw_lidar_points'].detach().cpu().numpy()[i]\n        lidar_pc = lidar_pc[:num_points, :]\n        \n        # bev lidar image\n        lidar_data = data['lidar'][i].detach().cpu().numpy().transpose(1,2,0)\n        lidar_data = (lidar_data * 255).astype(np.uint8)\n        \n        # MODEL PREDICTIONS\n        pred_waypoints = outputs['pred_wp'][i]\n        pred_waypoints[:, 1] *= -1 \n        pred_lanepoints = generate_lane_points(pred_waypoints)\n        \n        ## AUXILLARY TASK PREDICTIONS\n    \n        ## bounding boxes\n        pred_boxes = outputs['detections'][i]\n        pred_3d_boxes = convert_to_3d_bboxes(pred_boxes)\n        pred_corners_3d = boxes_to_corners_3d(pred_3d_boxes)\n        \n        # project to bev image space\n        pred_corners_2d = [get_rotated_bbox(bbox)[:, :2] for bbox in pred_boxes]\n        \n        ## depth information\n        pred_depth = (outputs['pred_depth'][i] * 255).astype(np.uint8)\n        pred_bev = outputs['pred_bev'][i].argmax(axis=0).astype(np.uint8)\n\n        # plot all data\n        visualizer.visualize_predictions(lidar_pc, pred_lanepoints, pred_corners_3d,\n                                         rgb_image, pred_depth, lidar_data, \n                                         pred_corners_2d = pred_corners_2d)\n\n        # save figure\n        visualizer.save_to_png(f\"Frame{frameIdx}.png\")\n        frameIdx +=1","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:28:50.927582Z","iopub.execute_input":"2024-07-22T08:28:50.928051Z","iopub.status.idle":"2024-07-22T08:36:58.467482Z","shell.execute_reply.started":"2024-07-22T08:28:50.928015Z","shell.execute_reply":"2024-07-22T08:36:58.465926Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"  0%|          | 0/48 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/mmdet/models/utils/gaussian_target.py:227: UserWarning:\n\n__floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n\n/opt/conda/lib/python3.10/site-packages/mmdet/models/utils/gaussian_target.py:229: UserWarning:\n\n__floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n\n100%|██████████| 48/48 [08:06<00:00, 10.13s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"def convert_images_to_video(images_dir, output_video_path, fps : int = 8):\n    input_images = [os.path.join(images_dir, *[x]) for x in sorted(os.listdir(images_dir)) if x.endswith('png')]\n    \n    if(len(input_images) > 0):\n        sample_image = cv2.imread(input_images[0])\n        height, width, _ = sample_image.shape\n        \n        # handles for input output videos\n        output_handle = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n    \n        # create progress bar\n        num_frames = int(len(input_images))\n        pbar = tqdm(total = num_frames, position=0, leave=True)\n\n        for i in tqdm(range(num_frames), position=0, leave=True):\n            frame = cv2.imread(input_images[i])\n            output_handle.write(frame)\n            pbar.update(1)\n\n        # release the output video handler\n        output_handle.release()\n                \n    else:\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:36:58.469551Z","iopub.execute_input":"2024-07-22T08:36:58.470004Z","iopub.status.idle":"2024-07-22T08:36:58.480770Z","shell.execute_reply.started":"2024-07-22T08:36:58.469964Z","shell.execute_reply":"2024-07-22T08:36:58.479465Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"FPS = 6\nconvert_images_to_video('./', f'scenario1_route0_{model.pred_len}pts_{FPS}fps.mp4', fps=FPS)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T08:36:58.482496Z","iopub.execute_input":"2024-07-22T08:36:58.482997Z","iopub.status.idle":"2024-07-22T08:37:00.396302Z","shell.execute_reply.started":"2024-07-22T08:36:58.482954Z","shell.execute_reply":"2024-07-22T08:37:00.395121Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\nOpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n100%|██████████| 95/95 [00:01<00:00, 50.89it/s]\n100%|██████████| 95/95 [00:01<00:00, 50.58it/s]\n","output_type":"stream"}]}]}