{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# dl imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARLA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 171.96it/s]\n",
      "There are 98 samples in Demo dataset\n"
     ]
    }
   ],
   "source": [
    "from config import GlobalConfig\n",
    "from data import CARLA_Data\n",
    "\n",
    "root_dir = '/home/surya/Downloads/transfuser-2022/data/demo/scenario1/'\n",
    "config = GlobalConfig()\n",
    "demo_set = CARLA_Data(root=root_dir, config=config, routeKey='route0')\n",
    "print(f\"There are {len(demo_set)} samples in Demo dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch style dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader_demo = DataLoader(demo_set, shuffle=False, batch_size=2, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample data is of type <class 'dict'> and has following keys\n",
      "rgb [2, 3, 160, 704]\n",
      "bev [2, 160, 160]\n",
      "depth [2, 160, 704]\n",
      "semantic [2, 160, 704]\n",
      "speed [2]\n",
      "x_command [2]\n",
      "y_command [2]\n",
      "target_point [2, 2]\n",
      "target_point_image [2, 1, 256, 256]\n",
      "lidar [2, 2, 256, 256]\n",
      "label [2, 20, 7]\n",
      "ego_waypoint [2, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "sample_data = next(iter(dataloader_demo))\n",
    "print(f\"sample data is of type {type(sample_data)} and has following keys\")\n",
    "\n",
    "for k,v in sample_data.items():\n",
    "    print(k, list(v.shape))\n",
    "    \n",
    "del sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "from model import LidarCenterNet\n",
    "model = LidarCenterNet(config, device, config.backbone, image_architecture='regnety_032', \n",
    "                           lidar_architecture='regnety_032', use_velocity=False)\n",
    "model.to(device);\n",
    "model.config.debug = True\n",
    "\n",
    "model.eval();\n",
    "checkpt = torch.load('/home/surya/Downloads/transfuser-2022/model_ckpt/transfuser/transfuser_regnet032_seed1_39.pth', map_location=device)\n",
    "model.load_state_dict(checkpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# new_state_dict = OrderedDict()\n",
    "\n",
    "# for k,v in checkpt.items():\n",
    "#     new_key = k.replace(\"module.\", \"\")\n",
    "#     if new_key != '_model.lidar_encoder._model.stem.conv.weight':\n",
    "#         new_state_dict[new_key] = v\n",
    "# torch.save(new_state_dict, 'transfuser_regnet032_seed1_39.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_waypoints_to_image(waypoints):\n",
    "    x = 128\n",
    "    y = 256\n",
    "    yaw = 0\n",
    "    c, s = np.cos(yaw), np.sin(yaw)\n",
    "    # use y x because coordinate is changed\n",
    "    r1_to_world = np.array([[c, -s, x], [s, c, y], [0, 0, 1]])\n",
    "\n",
    "    # convert to image space\n",
    "    # need to negate y componet as we do for lidar points\n",
    "    # we directly construct points in the image coordiante\n",
    "    # for lidar, forward +x, right +y\n",
    "    points = waypoints.copy()\n",
    "    points[:, 0] *= -1\n",
    "    points = points * 8\n",
    "    points = points[:, [1, 0]]\n",
    "    points = np.concatenate((points, np.ones_like(points[:, :1])), axis=-1)\n",
    "    points = r1_to_world @ points.T\n",
    "    points = (points.T)[:, :2].astype(np.int32)\n",
    "    points = np.clip(points, 0, 256)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotated_bbox(bbox):\n",
    "    x, y, w, h, yaw, _, _  =  bbox\n",
    "\n",
    "    bbox = np.array([[h,   w, 1],\n",
    "                     [h,  -w, 1],\n",
    "                     [-h, -w, 1],\n",
    "                     [-h,  w, 1],\n",
    "                     [0, 0, 1],\n",
    "                ])\n",
    "    \n",
    "    # The height and width of the bounding box value was changed by this factor \n",
    "    # during data collection. Fix that for future datasets and remove    \n",
    "    bbox[:, :2] /= 2\n",
    "    bbox[:, :2] = bbox[:, [1, 0]]\n",
    "\n",
    "    c, s = np.cos(yaw), np.sin(yaw)\n",
    "    # use y x because coordinate is changed\n",
    "    r1_to_world = np.array([[c, -s, x], [s, c, y], [0, 0, 1]])\n",
    "    bbox = r1_to_world @ bbox.T\n",
    "    bbox = bbox.T\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def draw_bounding_box(ax, bbox, **kwargs):\n",
    "    lns = []\n",
    "    lns.append(ax.plot( [bbox[0,0], bbox[1,0]], [bbox[0,1], bbox[1,1]], **kwargs)[0])\n",
    "    lns.append(ax.plot( [bbox[1,0], bbox[2,0]], [bbox[1,1], bbox[2,1]], **kwargs)[0])\n",
    "    lns.append(ax.plot( [bbox[2,0], bbox[3,0]], [bbox[2,1], bbox[3,1]], **kwargs)[0])\n",
    "    lns.append(ax.plot( [bbox[3,0], bbox[0,0]], [bbox[3,1], bbox[0,1]], **kwargs)[0])\n",
    "    return lns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_attention_map(attn_map, reshape_size):\n",
    "    attn_map = attn_map.sum(axis=0)\n",
    "    attn_map = attn_map / attn_map.sum(axis=1)[:, np.newaxis]\n",
    "    up_attn = cv2.resize(attn_map, reshape_size)\n",
    "    smoothed_attn_map = sp.ndimage.filters.gaussian_filter(up_attn, [1,1], mode='constant')\n",
    "    return smoothed_attn_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    def __init__(self):\n",
    "        self.fig = plt.figure(figsize=(20, 10))\n",
    "        self.fig.suptitle(\"Transfuser output\")\n",
    "        self.fig.tight_layout()\n",
    "        gs = self.fig.add_gridspec(3, 3,  height_ratios = [2,1,1], width_ratios = [2,1,1])\n",
    "        self.rgb_axes = self.fig.add_subplot(gs[0, :])\n",
    "        self.depth_axes = self.fig.add_subplot(gs[1, 0])\n",
    "        self.semantic_axes = self.fig.add_subplot(gs[2, 0])\n",
    "        self.bev_axes = self.fig.add_subplot(gs[1:, 1])\n",
    "        self.bbox_axes = self.fig.add_subplot(gs[1:, 2])\n",
    "        \n",
    "        self.color_code = np.array([\n",
    "            [0, 0, 0],       # black\n",
    "            [128, 128, 128], # grey\n",
    "            [255, 255, 0]    # yellow\n",
    "        ])\n",
    "    \n",
    "    def plotData(self, rgb_image, depth_image, semantic_image, bev_image,\n",
    "                 lidar_data, tgt_points, gt_boxes, \n",
    "                 pred_points = None, pred_boxes=None, \n",
    "                 img_attn_map = None, lidar_attn_map = None):\n",
    "        \n",
    "        # clear previous data before plotting\n",
    "        self.clearAxes()\n",
    "        \n",
    "        # rgb image with attention map\n",
    "        self.rgb_axes.imshow(rgb_image)\n",
    "        if img_attn_map is not None:\n",
    "            self.rgb_axes.imshow(smooth_img_attn_map, cmap='inferno', alpha=0.3)\n",
    "        self.rgb_axes.set(xticks=[], yticks=[])\n",
    "        \n",
    "        # depth image\n",
    "        self.depth_axes.imshow(depth_image)\n",
    "        self.depth_axes.set(xticks=[], yticks=[])\n",
    "        \n",
    "        # semantic segmentation\n",
    "        self.semantic_axes.imshow(semantic_image)\n",
    "        self.semantic_axes.set(xticks=[], yticks=[])\n",
    "        \n",
    "        # BEV class prediction\n",
    "        self.bev_axes.imshow(self.color_code[bev_image])\n",
    "        self.bev_axes.set(xticks=[], yticks=[])\n",
    "        \n",
    "        # lidar data with waypoints\n",
    "        self.bbox_axes.imshow(lidar_data)\n",
    "        self.bbox_axes.set(xticks=[], yticks=[])\n",
    "        self.bbox_axes.plot(tgt_points[:,0], tgt_points[:,1], 'go', linewidth =3)\n",
    "        if pred_points is not None:\n",
    "            self.bbox_axes.plot(pred_points[:,0], pred_points[:,1], 'ro', linewidth =3)\n",
    "        \n",
    "        # bounding boxes\n",
    "        for bbox in gt_boxes:\n",
    "            draw_bounding_box(self.bbox_axes, bbox, color='white')\n",
    "        \n",
    "        if pred_boxes is not None:\n",
    "            for bbox in pred_boxes:\n",
    "                draw_bounding_box(self.bbox_axes, bbox, color='cyan')\n",
    "        \n",
    "        # lidar attention map\n",
    "        if lidar_attn_map is not None:\n",
    "            self.bbox_axes.imshow(lidar_attn_map, cmap='inferno', alpha=0.3)\n",
    "\n",
    "        self.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    def clearAxes(self):\n",
    "        self.rgb_axes.clear()\n",
    "        self.depth_axes.clear()\n",
    "        self.semantic_axes.clear()\n",
    "        self.bev_axes.clear()\n",
    "        self.bbox_axes.clear()\n",
    "        \n",
    "    def saveFigure(self, outputPath):\n",
    "        self.fig.savefig(outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:00<?, ?it/s]/home/surya/miniconda3/envs/tfuse/lib/python3.7/site-packages/mmdet/models/utils/gaussian_target.py:227: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  topk_clses = topk_inds // (height * width)\n",
      "/home/surya/miniconda3/envs/tfuse/lib/python3.7/site-packages/mmdet/models/utils/gaussian_target.py:229: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  topk_ys = topk_inds // width\n",
      "100%|██████████| 49/49 [01:57<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "visualizer = Visualizer()\n",
    "\n",
    "frameIdx = 0\n",
    "\n",
    "for data in tqdm(dataloader_demo):\n",
    "\n",
    "    # load data to device, according to type\n",
    "    for k in ['rgb', 'depth', 'lidar', 'label', 'ego_waypoint', \\\n",
    "              'target_point', 'target_point_image', 'speed']:\n",
    "        data[k] = data[k].to(device, torch.float32)\n",
    "    for k in ['semantic', 'bev']:\n",
    "        data[k] = data[k].to(device, torch.long)\n",
    "    \n",
    "    # get model predictions\n",
    "    _, outputs = model(data)\n",
    "\n",
    "    # iterate through each sample in batch \n",
    "    bs = data['rgb'].shape[0]\n",
    "    for i in range(bs):\n",
    "        # extract input data\n",
    "        rgb_image = data['rgb'][i].permute(1, 2, 0).detach().cpu().numpy().astype(np.uint8)\n",
    "        lidar_data = data['lidar'][i].detach().cpu().numpy().transpose(1,2,0)\n",
    "        tgt_waypoints = data['ego_waypoint'][i].detach().cpu().numpy()\n",
    "        tgt_waypoints_image = convert_waypoints_to_image(tgt_waypoints)\n",
    "        gt_boxes = data['label'][i].detach().cpu().numpy()\n",
    "        gt_boxes = gt_boxes[gt_boxes.sum(axis=-1) != 0.]\n",
    "        rotated_bboxes_gt = [get_rotated_bbox(bbox)[:, :2] for bbox in gt_boxes]\n",
    "\n",
    "        # extract model predictions\n",
    "        pred_depth = outputs['pred_depth'][i]\n",
    "        indices = np.argmax(outputs['pred_semantic'], axis=1)\n",
    "        pred_semantic = np.array(config.classes_list)[indices[i, ...], ...].astype('uint8')        \n",
    "        pred_bev = outputs['pred_bev'][i].argmax(axis=0).astype(np.uint8)\n",
    "        pred_waypoints = outputs['pred_wp'][i]\n",
    "        pred_waypoints_image = convert_waypoints_to_image(pred_waypoints)\n",
    "        pred_boxes = outputs['detections'][i]\n",
    "        rotated_bboxes_pred = [get_rotated_bbox(bbox)[:, :2] for bbox in pred_boxes]\n",
    "        attn_map = np.sum(outputs['attn_map'][i], axis=0)\n",
    "        image_attn_map = attn_map[0:110, 0:110].reshape(110, 5, 22)\n",
    "        smooth_img_attn_map = smooth_attention_map(image_attn_map, reshape_size=(704, 160))\n",
    "        lidar_attn_map = attn_map[110:, 110:].reshape(64, 8, 8)\n",
    "        smooth_lidar_attn_map = smooth_attention_map(lidar_attn_map, reshape_size=(256, 256))\n",
    "\n",
    "        # plot all data\n",
    "        visualizer.plotData(rgb_image, depth_image = pred_depth, semantic_image=pred_semantic, \n",
    "            bev_image = pred_bev, lidar_data = lidar_data, tgt_points = tgt_waypoints_image, \n",
    "            gt_boxes = rotated_bboxes_gt, pred_points = pred_waypoints_image, \n",
    "            pred_boxes = rotated_bboxes_pred, \n",
    "            #img_attn_map = smooth_img_attn_map, \n",
    "            #lidar_attn_map = smooth_lidar_attn_map\n",
    "       )\n",
    "\n",
    "        # save figure\n",
    "        visualizer.saveFigure(f\"outputFolder/Frame{frameIdx}.png\")\n",
    "        frameIdx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
