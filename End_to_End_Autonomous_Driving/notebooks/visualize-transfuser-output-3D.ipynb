{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# dl imports\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARLA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 151.79it/s]\n",
      "There are 95 samples in Demo dataset\n"
     ]
    }
   ],
   "source": [
    "from config import GlobalConfig\n",
    "from data import CARLA_Data\n",
    "\n",
    "root_dir = '/home/surya/Downloads/transfuser-2022/data/demo/scenario1/'\n",
    "config = GlobalConfig()\n",
    "demo_set = CARLA_Data(root=root_dir, config=config, routeKey='route2', load_raw_lidar=True)\n",
    "print(f\"There are {len(demo_set)} samples in Demo dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch style dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader_demo = DataLoader(demo_set, shuffle=False, batch_size=2, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample data is of type <class 'dict'> and has following keys\n",
      "rgb [2, 3, 160, 704]\n",
      "bev [2, 160, 160]\n",
      "depth [2, 160, 704]\n",
      "semantic [2, 160, 704]\n",
      "speed [2]\n",
      "x_command [2]\n",
      "y_command [2]\n",
      "target_point [2, 2]\n",
      "target_point_image [2, 1, 256, 256]\n",
      "raw_lidar [2, 10000, 3]\n",
      "num_raw_lidar_points [2]\n",
      "lidar [2, 2, 256, 256]\n",
      "label [2, 20, 7]\n",
      "ego_waypoint [2, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "sample_data = next(iter(dataloader_demo))\n",
    "print(f\"sample data is of type {type(sample_data)} and has following keys\")\n",
    "\n",
    "for k,v in sample_data.items():\n",
    "    print(k, list(v.shape))\n",
    "    \n",
    "del sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "from model import LidarCenterNet\n",
    "model = LidarCenterNet(config, device, config.backbone, image_architecture='regnety_032', \n",
    "                           lidar_architecture='regnety_032', use_velocity=False)\n",
    "model.to(device);\n",
    "model.config.debug = True\n",
    "\n",
    "model.eval();\n",
    "checkpt = torch.load('/home/surya/Downloads/transfuser-2022/model_ckpt/transfuser/transfuser_regnet032_seed1_39.pth', map_location=device)\n",
    "model.load_state_dict(checkpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import VEHICLE_TO_LIDAR_FWD, LIDAR_HEIGHT\n",
    "\n",
    "def generate_lane_points(waypoints, lane_width = 1.0):\n",
    "    # input waypoints are 2d coordinates of centerline (N,2)\n",
    "    # this function generates left and right lane corners\n",
    "    # by subtracting and adding half lane width. We convert\n",
    "    # to 3D Lidar coordinates by placing points at ground level\n",
    "\n",
    "    n_points = waypoints.shape[0]\n",
    "    lane_points = np.zeros((n_points * 2 , 3))\n",
    "    \n",
    "    # vehicle to lidar frame\n",
    "    lane_points[:n_points, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n",
    "    lane_points[n_points:, 0] = waypoints[:,0] + VEHICLE_TO_LIDAR_FWD\n",
    "    \n",
    "    # left and right lanes\n",
    "    lane_points[:n_points,1] = waypoints[:,1] - (lane_width * 0.5)\n",
    "    lane_points[n_points:,1] = waypoints[:,1] + (lane_width * 0.5)\n",
    "    \n",
    "    # fixed height\n",
    "    lane_points[:,2] = -LIDAR_HEIGHT\n",
    "    return lane_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev_to_lidar = np.array([\n",
    "            [0, -(1/8.0), 32],\n",
    "            [-(1/8.0), 0, 16],\n",
    "            [0 , 0, 1]\n",
    "])\n",
    "\n",
    "\n",
    "def convert_to_3d_bboxes(boxes_2d):\n",
    "    n_boxes = boxes_2d.shape[0]\n",
    "    bbox_3d = np.zeros((n_boxes, 7))\n",
    "\n",
    "    # xy position from bev pixels to metres\n",
    "    homogenous_coordinates = np.hstack([boxes_2d[:, :2], np.ones((n_boxes, 1))])\n",
    "    bbox_3d[:, :2] = (bev_to_lidar @ homogenous_coordinates.T).T[:, :2]\n",
    "    bbox_3d[:, 3] = boxes_2d[:, 3] / 8  # length \n",
    "    bbox_3d[:, 4] = boxes_2d[:, 2] / 8  # width\n",
    "    bbox_3d[:, 6] = boxes_2d[:, 4]      # yaw\n",
    "\n",
    "    # hardcoding z values\n",
    "    bbox_3d[:, 2] = -1.25\n",
    "    bbox_3d[:, 5] = 2.5\n",
    "    return bbox_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCD_CAM_VIEW = dict(\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            eye=dict(x=-0.9, y=0, z=0.2)\n",
    "    )\n",
    "\n",
    "PCD_SCENE=dict(\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        zaxis=dict(visible=False,),\n",
    "        aspectmode='manual',\n",
    "        aspectratio=dict(x=1, y=1, z=0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_lidar3d_plots, get_image2d_plots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "class Visualizer:\n",
    "    def __init__(self, model_name, fig_width=1000, fig_height=800, pred_box_color='orange', \n",
    "                 waypoints_color = 'red', scene=PCD_SCENE, cam_view=PCD_CAM_VIEW):\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Create a 2x1 figure, top row for point cloud data\n",
    "        # bottom row for rgb image data\n",
    "        self.fig = make_subplots(rows=2, cols=1,specs=[[{\"type\": \"scatter3d\"}], [{}]], row_heights=[0.65, 0.35],\n",
    "                                horizontal_spacing=0.0, vertical_spacing = 0.0)\n",
    "        self.fig.update_layout(template=\"plotly_dark\", scene=scene, scene_camera = cam_view,\n",
    "                height = fig_height, width = fig_width, autosize=False,\n",
    "                title=f\"END TO END AUTONOMOUS DRIVING {self.model_name}\", title_x=0.5, title_y=0.95,\n",
    "                margin=dict(r=0, b=0, l=0, t=0))            \n",
    "        self.fig.update_xaxes(showticklabels=False, visible=False, row=2, col=1)\n",
    "        self.fig.update_yaxes(showticklabels=False, visible=False, row=2, col=1)\n",
    "        \n",
    "        # set export image option\n",
    "        self.fig.to_image(format=\"png\", engine=\"kaleido\")\n",
    "        self.pred_color = pred_box_color\n",
    "        self.waypoints_color = waypoints_color\n",
    "    \n",
    "    def clear_figure_data(self):\n",
    "        self.fig.data = []\n",
    "    \n",
    "    def get_bbox_colors(self, bbox_corners):\n",
    "        return [self.pred_color] * bbox_corners.shape[0] if bbox_corners is not None else None\n",
    "        \n",
    "    def plot_waypoints(self, waypoints):\n",
    "        return go.Mesh3d(x=waypoints[:,0], y=waypoints[:,1], z=waypoints[:,2], \n",
    "                         opacity=0.4, color=self.waypoints_color, \n",
    "                         hoverinfo='skip',showlegend=False)\n",
    "\n",
    "    def add_lidar_plots(self, points, waypoints, pred_corners=None):\n",
    "        lidar_3d_plots = get_lidar3d_plots(points, pc_kwargs=dict(colorscale='viridis', marker_size=1.2),\n",
    "                                   pred_box_corners = pred_corners, \n",
    "                                   pred_box_colors = self.get_bbox_colors(pred_corners))\n",
    "        lidar_3d_plots.append(self.plot_waypoints(waypoints))\n",
    "        for trace in lidar_3d_plots:\n",
    "            self.fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "    def add_image_plots(self, image):\n",
    "        self.fig.add_trace(get_image2d_plots(rgb_image = image), row=2, col=1)\n",
    "            \n",
    "    def visualize_predictions(self, points, waypoints, image, pred_corners=None):\n",
    "        # clear previous data and plot lidar, image data\n",
    "        self.clear_figure_data()\n",
    "        self.add_lidar_plots(points=points, waypoints=waypoints, pred_corners=pred_corners)\n",
    "        self.add_image_plots(image=image)\n",
    "    \n",
    "    def show_figure(self):\n",
    "        self.fig.show()\n",
    "        \n",
    "    def save_to_png(self, output_path):\n",
    "        self.fig.write_image(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]/home/surya/miniconda3/envs/tfuse/lib/python3.7/site-packages/mmdet/models/utils/gaussian_target.py:227: UserWarning:\n",
      "\n",
      "__floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "\n",
      "/home/surya/miniconda3/envs/tfuse/lib/python3.7/site-packages/mmdet/models/utils/gaussian_target.py:229: UserWarning:\n",
      "\n",
      "__floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "\n",
      "  0%|          | 0/48 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib agg\n",
    "visualizer = Visualizer(model_name='TRANSFUSER')\n",
    "\n",
    "from utils import boxes_to_corners_3d\n",
    "\n",
    "frameIdx = 0\n",
    "for data in tqdm(dataloader_demo):\n",
    "\n",
    "    # load data to device, according to type\n",
    "    for k in ['rgb', 'depth', 'lidar', 'label', 'ego_waypoint', \\\n",
    "              'target_point', 'target_point_image', 'speed']:\n",
    "        data[k] = data[k].to(device, torch.float32)\n",
    "    for k in ['semantic', 'bev']:\n",
    "        data[k] = data[k].to(device, torch.long)\n",
    "    \n",
    "    # get model predictions\n",
    "    _, outputs = model(data)\n",
    "\n",
    "    # iterate through each sample in batch \n",
    "    bs = data['rgb'].shape[0]\n",
    "    for i in range(bs):\n",
    "        # extract input data\n",
    "        rgb_image = data['rgb'][i].permute(1, 2, 0).detach().cpu().numpy().astype(np.uint8)\n",
    "        tgt_waypoints = data['ego_waypoint'][i].detach().cpu().numpy()\n",
    "        \n",
    "        lidar_pc = data['raw_lidar'][i].detach().cpu().numpy()\n",
    "        num_points = data['num_raw_lidar_points'].detach().cpu().numpy()[i]\n",
    "        lidar_pc = lidar_pc[:num_points, :]\n",
    "\n",
    "        # extract model predictions\n",
    "        pred_waypoints = outputs['pred_wp'][i]\n",
    "        pred_lanepoints = generate_lane_points(pred_waypoints)\n",
    "        \n",
    "        pred_boxes = outputs['detections'][i]\n",
    "        pred_3d_boxes = convert_to_3d_bboxes(pred_boxes)\n",
    "        pred_corners = boxes_to_corners_3d(pred_3d_boxes)\n",
    "\n",
    "        # plot all data\n",
    "        visualizer.visualize_predictions(lidar_pc, waypoints=pred_lanepoints, image=rgb_image)\n",
    "\n",
    "        # save figure\n",
    "        visualizer.save_to_png(f\"testFolder/Frame{frameIdx}.png\")\n",
    "        frameIdx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_images_to_video(images_dir, output_video_path, fps : int = 20):\n",
    "    input_images = [os.path.join(images_dir, *[x]) for x in sorted(os.listdir(images_dir)) if x.endswith('png')]\n",
    "    \n",
    "    if(len(input_images) > 0):\n",
    "        sample_image = cv2.imread(input_images[0])\n",
    "        height, width, _ = sample_image.shape\n",
    "        \n",
    "        # handles for input output videos\n",
    "        output_handle = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "\n",
    "        # create progress bar\n",
    "        num_frames = int(len(input_images))\n",
    "        pbar = tqdm(total = num_frames, position=0, leave=True)\n",
    "\n",
    "        for i in tqdm(range(num_frames), position=0, leave=True):\n",
    "            frame = cv2.imread(input_images[i])\n",
    "            output_handle.write(frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "        # release the output video handler\n",
    "        output_handle.release()\n",
    "                \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_images_to_video('testFolder/', 'scenario1_route2.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
