m_runtime = std::unique_ptr<nvinfer1::IRuntime>{nvinfer1::createInferRuntime(m_logger)};

// buffer is trt engine
m_engine = std::unique_ptr<nvinfer1::ICudaEngine>(m_runtime->deserializeCudaEngine(buffer.data(), buffer.size()));

m_context = std::unique_ptr<nvinfer1::IExecutionContext>(m_engine->createExecutionContext());

clearGpuBuffers();
m_buffers.resize(m_engine->getNbIOTensors());

// create cuda streams
cudaStream_t stream;
Util::checkCudaErrorCode(cudaStreamCreate(&stream));

// allocate memory for input, output buffers
Util::checkCudaErrorCode(cudaMallocAsync(&m_buffers[i], outputLength * m_options.maxBatchSize * sizeof(T), stream));

// Synchronize and destroy the cuda stream
Util::checkCudaErrorCode(cudaStreamSynchronize(stream));
Util::checkCudaErrorCode(cudaStreamDestroy(stream));

////////////////////////////////////// inference

// Create the cuda stream that will be used for inference
cudaStream_t inferenceCudaStream;
Util::checkCudaErrorCode(cudaStreamCreate(&inferenceCudaStream));


bool status = m_context->enqueueV3(inferenceCudaStream);

// copy data from GPU to CPU
Util::checkCudaErrorCode(cudaMemcpyAsync(output.data(),
                                     static_cast<char *>(m_buffers[outputBinding]) + (batch * sizeof(T) * outputLength),
                                     outputLength * sizeof(T), cudaMemcpyDeviceToHost, inferenceCudaStream));

// Synchronize the cuda stream
Util::checkCudaErrorCode(cudaStreamSynchronize(inferenceCudaStream));
Util::checkCudaErrorCode(cudaStreamDestroy(inferenceCudaStream));

