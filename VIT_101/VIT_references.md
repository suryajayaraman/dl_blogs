# Vision Transformers 101

## Research papers
- [Attention Is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- [ViT official paper](https://arxiv.org/abs/2010.11929)
- [Attention Mechanisms in Computer Vision: A Survey](https://arxiv.org/pdf/2111.07624)
- [DAT++](https://arxiv.org/pdf/2309.01430)


## Blog posts / Lectures
- [Attention mechanism in computer vision](https://blog.paperspace.com/attention-mechanisms-in-computer-vision-cbam/)
- [Layer Normalization](https://www.youtube.com/watch?v=2V3Uduw1zwQ)
- [QKV explained](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)


## Repositories / code
- [Vision Attention](https://github.com/MenghaoGuo/Awesome-Vision-Attentions?tab=readme-ov-file)
- [ViT pytorch](https://github.com/lucidrains/vit-pytorch)
- [Deformable attention](https://github.com/lucidrains/deformable-attention)
- [Vision Transformer with Deformable attention](https://github.com/LeapLabTHU/DAT)
- [ViT pytorch official implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py)