{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1617bd68",
   "metadata": {},
   "source": [
    "## Vision Transformers\n",
    "- Paper link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0211bf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d23e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb428a6",
   "metadata": {},
   "source": [
    "### CONSTANTS\n",
    "\n",
    "We'll be defining some hyperparmeters for the model, (correspond to VIT-tiny model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b35ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "patch_size = 16   # image is divided into patches of 16 x16 pixels\n",
    "num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "\n",
    "# transformer parameters\n",
    "embed_dim = 192\n",
    "num_heads = 3\n",
    "depth = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f7e89",
   "metadata": {},
   "source": [
    "## Vision Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4f574",
   "metadata": {},
   "source": [
    "- Only Encoder from original Transformer paper\n",
    "- Cover different modules in vision Transformers\n",
    "    - Patch embedding\n",
    "    - Cls Token, Positional embedding\n",
    "    - Layer Normalization\n",
    "    - Self-Attention \n",
    "    - MLP Layer\n",
    "    - Transformer block\n",
    "    - Putting it together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc9f47",
   "metadata": {},
   "source": [
    "### Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f873ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Generate Patches from input image, create embeddings for each patch \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244904e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embedding output shape = torch.Size([1, 196, 192])\n"
     ]
    }
   ],
   "source": [
    "# (Batch size, RGB channels, height, width)\n",
    "input_data = torch.Tensor(1, 3, img_size, img_size)\n",
    "patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=3, embed_dim=embed_dim)\n",
    "patch_embed_output = patch_embed(input_data)\n",
    "print(f\"Patch embedding output shape = {patch_embed_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1112a5d",
   "metadata": {},
   "source": [
    "### Cls Token, Position Embedding\n",
    "- Need a mechanism to inform the model of \n",
    "    - Embedding class\n",
    "    - Order of patches\n",
    "- Positional embeddings can be either\n",
    "    - Derived from mathematical function (usually involves sines and cosines of input index)\n",
    "    - Learnt by model (which is current case)\n",
    "    \n",
    "- **Need for Classification token?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b72cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class token output shape = torch.Size([1, 197, 192])\n"
     ]
    }
   ],
   "source": [
    "# add class token for each patch\n",
    "cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "cls_token = cls_token.expand(input_data.shape[0], -1, -1)\n",
    "cls_token_output = torch.cat((cls_token, patch_embed_output), dim=1)\n",
    "print(f\"Class token output shape = {cls_token_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c0fb3",
   "metadata": {},
   "source": [
    "**Note the usage of nn.Parameter, and not something like nn.Linear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69cba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Embedding output shape = torch.Size([1, 197, 192])\n"
     ]
    }
   ],
   "source": [
    "# add positional encoding to each token\n",
    "pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "\n",
    "# To align with original architecture, we need to add \n",
    "# dropout on the position embedding. For now, we'll define\n",
    "# dropout probability as 0\n",
    "pos_drop = nn.Dropout(p=0)\n",
    "\n",
    "pos_embed_output = cls_token_output + pos_drop(pos_embed)\n",
    "print(f\"Position Embedding output shape = {pos_embed_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba032a",
   "metadata": {},
   "source": [
    "### Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764120f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, attn_drop, proj_drop):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x) # (1, 197, 3 * 192), contains query, key and values\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, C // self.num_heads)  # (1, 197, 3, 3, 64)\n",
    "        \n",
    "        # (qkv, batch_size, num_heads, num_tokens, dim_per_head)\n",
    "        qkv =qkv.permute(2, 0, 3, 1, 4)  # (3, 1, 3, 197, 64)\n",
    "        \n",
    "        # query, key, values all are of (1,3,197,64) shape\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # query (1,3,197,64) * key (1,3,64, 197) = (1,3,197,197)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # attn (1,3,197,197) * value (1,3,197,64) = (1,3,197,64)\n",
    "        x = (attn @ v)\n",
    "        \n",
    "        # (1,3,197,64) -> (1,197,3,64) -> (1,197,192)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        \n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f54a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape = torch.Size([1, 197, 192])\n",
      "Attention shape = torch.Size([1, 3, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "attention_module = Attention(dim = embed_dim, num_heads = num_heads, attn_drop =0.0, proj_drop =0.0 )\n",
    "attn_output, attn = attention_module(pos_embed_output)\n",
    "print(f\"Attention output shape = {attn_output.shape}\")\n",
    "print(f\"Attention shape = {attn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86a287",
   "metadata": {},
   "source": [
    "### MLP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f96f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hid_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hid_dim, out_dim)\n",
    "        self.drop = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6694e",
   "metadata": {},
   "source": [
    "### Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d70195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, attn_drop=0, proj_drop=0)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_dim=dim, hid_dim=mlp_hidden_dim, out_dim=dim)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557eca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer block output shape = torch.Size([1, 197, 192])\n"
     ]
    }
   ],
   "source": [
    "transformer_block = Block(dim=embed_dim, num_heads=3, mlp_ratio=4)\n",
    "transformer_block_output, _ = transformer_block(pos_embed_output)\n",
    "print(f\"Transformer block output shape = {transformer_block_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943b0ce",
   "metadata": {},
   "source": [
    "## Visualize Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafb90b",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557c0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
