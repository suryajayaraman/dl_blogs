{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1617bd68",
   "metadata": {},
   "source": [
    "## Deformable Attention\n",
    "- [DAT++: Spatially Dynamic Vision Transformer with Deformable Attention](https://arxiv.org/pdf/2309.01430)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0211bf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65323aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install natten, timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d23e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import einops\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn.functional import pad\n",
    "# from torch.nn.init import trunc_normal_\n",
    "\n",
    "# from timm.models.layers import to_2tuple, trunc_normal_,  DropPath\n",
    "# from natten.functional import NATTEN2DQKRPBFunction, NATTEN2DAVFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f7e89",
   "metadata": {},
   "source": [
    "## Deformable Attention Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4f574",
   "metadata": {},
   "source": [
    "- xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc9f47",
   "metadata": {},
   "source": [
    "### Deformable Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f873ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAttentionBaseline(nn.Module):\n",
    "    def __init__(self, q_size, kv_size, n_heads, n_head_channels, n_groups,\n",
    "        attn_drop, proj_drop, stride,\n",
    "        no_off, ksize):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_head_channels = n_head_channels\n",
    "        self.scale = self.n_head_channels ** -0.5\n",
    "        self.n_heads = n_heads\n",
    "        self.q_h, self.q_w = q_size\n",
    "        self.kv_h, self.kv_w = self.q_h // stride, self.q_w // stride\n",
    "        self.nc = n_head_channels * n_heads\n",
    "        self.n_groups = n_groups\n",
    "        self.n_group_channels = self.nc // self.n_groups\n",
    "        self.n_group_heads = self.n_heads // self.n_groups\n",
    "        self.no_off = no_off\n",
    "        self.ksize = ksize\n",
    "        self.stride = stride\n",
    "        kk = self.ksize\n",
    "        pad_size = kk // 2 if kk != stride else 0\n",
    "\n",
    "        self.conv_offset = nn.Sequential(\n",
    "            nn.Conv2d(self.n_group_channels, self.n_group_channels, kk, stride, pad_size, groups=self.n_group_channels),\n",
    "            LayerNormProxy(self.n_group_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(self.n_group_channels, 2, 1, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "        self.proj_q = nn.Conv2d(self.nc, self.nc,kernel_size=1, stride=1, padding=0)\n",
    "        self.proj_k = nn.Conv2d(self.nc, self.nc, kernel_size=1, stride=1, padding=0)\n",
    "        self.proj_v = nn.Conv2d(self.nc, self.nc, kernel_size=1, stride=1, padding=0)\n",
    "        self.proj_out = nn.Conv2d(self.nc, self.nc, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop, inplace=True)\n",
    "        self.attn_drop = nn.Dropout(attn_drop, inplace=True)\n",
    "\n",
    "        self.rpe_table = nn.Parameter(\n",
    "            torch.zeros(self.n_heads, self.q_h * 2 - 1, self.q_w * 2 - 1)\n",
    "        )\n",
    "        trunc_normal_(self.rpe_table, std=0.01)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_ref_points(self, H_key, W_key, B, dtype, device):\n",
    "\n",
    "        ref_y, ref_x = torch.meshgrid(\n",
    "            torch.linspace(0.5, H_key - 0.5, H_key, dtype=dtype, device=device),\n",
    "            torch.linspace(0.5, W_key - 0.5, W_key, dtype=dtype, device=device),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        ref = torch.stack((ref_y, ref_x), -1)\n",
    "        ref[..., 1].div_(W_key - 1.0).mul_(2.0).sub_(1.0)\n",
    "        ref[..., 0].div_(H_key - 1.0).mul_(2.0).sub_(1.0)\n",
    "        ref = ref[None, ...].expand(B * self.n_groups, -1, -1, -1) # B * g H W 2\n",
    "\n",
    "        return ref\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _get_q_grid(self, H, W, B, dtype, device):\n",
    "\n",
    "        ref_y, ref_x = torch.meshgrid(\n",
    "            torch.arange(0, H, dtype=dtype, device=device),\n",
    "            torch.arange(0, W, dtype=dtype, device=device),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        ref = torch.stack((ref_y, ref_x), -1)\n",
    "        ref[..., 1].div_(W - 1.0).mul_(2.0).sub_(1.0)\n",
    "        ref[..., 0].div_(H - 1.0).mul_(2.0).sub_(1.0)\n",
    "        ref = ref[None, ...].expand(B * self.n_groups, -1, -1, -1) # B * g H W 2\n",
    "\n",
    "        return ref\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, C, H, W = x.size()\n",
    "        dtype, device = x.dtype, x.device\n",
    "\n",
    "        q = self.proj_q(x)\n",
    "        q_off = einops.rearrange(q, 'b (g c) h w -> (b g) c h w', g=self.n_groups, c=self.n_group_channels)\n",
    "        offset = self.conv_offset(q_off).contiguous()  # B * g 2 Hg Wg\n",
    "        Hk, Wk = offset.size(2), offset.size(3)\n",
    "        n_sample = Hk * Wk\n",
    "\n",
    "        offset = einops.rearrange(offset, 'b p h w -> b h w p')\n",
    "        reference = self._get_ref_points(Hk, Wk, B, dtype, device)\n",
    "\n",
    "        pos = (offset + reference).clamp(-1., +1.)\n",
    "\n",
    "        x_sampled = F.grid_sample(\n",
    "            input=x.reshape(B * self.n_groups, self.n_group_channels, H, W), \n",
    "            grid=pos[..., (1, 0)], # y, x -> x, y\n",
    "            mode='bilinear', align_corners=True) # B * g, Cg, Hg, Wg\n",
    "                \n",
    "\n",
    "        x_sampled = x_sampled.reshape(B, C, 1, n_sample)\n",
    "\n",
    "        q = q.reshape(B * self.n_heads, self.n_head_channels, H * W)\n",
    "        k = self.proj_k(x_sampled).reshape(B * self.n_heads, self.n_head_channels, n_sample)\n",
    "        v = self.proj_v(x_sampled).reshape(B * self.n_heads, self.n_head_channels, n_sample)\n",
    "\n",
    "        attn = torch.einsum('b c m, b c n -> b m n', q, k) # B * h, HW, Ns\n",
    "        attn = attn.mul(self.scale)\n",
    "\n",
    "\n",
    "        rpe_table = self.rpe_table\n",
    "        rpe_bias = rpe_table[None, ...].expand(B, -1, -1, -1)\n",
    "        q_grid = self._get_q_grid(H, W, B, dtype, device)\n",
    "        displacement = (q_grid.reshape(B * self.n_groups, H * W, 2).unsqueeze(2) - pos.reshape(B * self.n_groups, n_sample, 2).unsqueeze(1)).mul(0.5)\n",
    "        attn_bias = F.grid_sample(\n",
    "            input=einops.rearrange(rpe_bias, 'b (g c) h w -> (b g) c h w', c=self.n_group_heads, g=self.n_groups),\n",
    "            grid=displacement[..., (1, 0)],\n",
    "            mode='bilinear', align_corners=True) # B * g, h_g, HW, Ns\n",
    "\n",
    "        attn_bias = attn_bias.reshape(B * self.n_heads, H * W, n_sample)\n",
    "        attn = attn + attn_bias\n",
    "\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = torch.einsum('b m n, b c n -> b c m', attn, v)\n",
    "\n",
    "        out = out.reshape(B, C, H, W)\n",
    "\n",
    "        y = self.proj_drop(self.proj_out(out))\n",
    "\n",
    "        return y, pos.reshape(B, self.n_groups, Hk, Wk, 2), reference.reshape(B, self.n_groups, Hk, Wk, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244904e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embedding output shape = torch.Size([1, 196, 384])\n"
     ]
    }
   ],
   "source": [
    "# (Batch size, RGB channels, height, width)\n",
    "input_data = torch.Tensor(1, 3, img_size, img_size)\n",
    "patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=3, embed_dim=embed_dim)\n",
    "patch_embed_output = patch_embed(input_data)\n",
    "print(f\"Patch embedding output shape = {patch_embed_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1112a5d",
   "metadata": {},
   "source": [
    "### Neighbourhood Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b72cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class token output shape = torch.Size([1, 197, 384])\n"
     ]
    }
   ],
   "source": [
    "class NeighborhoodAttention2D(nn.Module):\n",
    "    def __init__(self, dim, kernel_size, num_heads, attn_drop=0., proj_drop=0., dilation=None):\n",
    "        super().__init__()\n",
    "        self.fp16_enabled = False\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // self.num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.kernel_size = kernel_size\n",
    "        if type(dilation) is str:\n",
    "            self.dilation = None\n",
    "            self.window_size = None\n",
    "        else:\n",
    "            self.dilation = dilation or 1\n",
    "            self.window_size = self.kernel_size * self.dilation\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.rpb = nn.Parameter(torch.zeros(num_heads, (2 * kernel_size - 1), (2 * kernel_size - 1)))\n",
    "        trunc_normal_(self.rpb, std=.02, mean=0., a=-2., b=2.)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        B, Hp, Wp, C = x.shape\n",
    "        H, W = int(Hp), int(Wp)\n",
    "        pad_l = pad_t = pad_r = pad_b = 0\n",
    "        dilation = self.dilation\n",
    "        window_size = self.window_size\n",
    "        if window_size is None:\n",
    "            dilation = max(min(H, W) // self.kernel_size, 1)\n",
    "            window_size = dilation * self.kernel_size\n",
    "        if H < window_size or W < window_size:\n",
    "            pad_l = pad_t = 0\n",
    "            pad_r = max(0, window_size - W)\n",
    "            pad_b = max(0, window_size - H)\n",
    "            x = pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            _, H, W, _ = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, H, W, 3, self.num_heads, self.head_dim).permute(3, 0, 4, 1, 2, 5)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q = q * self.scale\n",
    "        # breakpoint()\n",
    "        attn = NATTEN2DQKRPBFunction.apply(q, k, self.rpb, self.kernel_size, dilation)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = NATTEN2DAVFunction.apply(attn, v, self.kernel_size, dilation)\n",
    "        x = x.permute(0, 2, 3, 1, 4).reshape(B, H, W, C)\n",
    "        if pad_r or pad_b:\n",
    "            x = x[:, :Hp, :Wp, :]\n",
    "\n",
    "        return self.proj_drop(self.proj(x)).permute(0, 3, 1, 2), None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69cba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Embedding output shape = torch.Size([1, 197, 384])\n"
     ]
    }
   ],
   "source": [
    "# # add positional encoding to each token\n",
    "# pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "\n",
    "# # To align with original architecture, we need to add \n",
    "# # dropout on the position embedding. For now, we'll define\n",
    "# # dropout probability as 0\n",
    "# pos_drop = nn.Dropout(p=0)\n",
    "\n",
    "# pos_embed_output = cls_token_output + pos_drop(pos_embed)\n",
    "# print(f\"Position Embedding output shape = {pos_embed_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba032a",
   "metadata": {},
   "source": [
    "### Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormProxy(nn.Module):\n",
    "    def __init__(self, dim):    \n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = einops.rearrange(x, 'b c h w -> b h w c')\n",
    "        x = self.norm(x)\n",
    "        return einops.rearrange(x, 'b h w c -> b c h w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611492bf",
   "metadata": {},
   "source": [
    "### MLP Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMLPWithConv(nn.Module):\n",
    "    def __init__(self, channels, expansion, drop):\n",
    "        super().__init__()\n",
    "        self.dim1 = channels\n",
    "        self.dim2 = channels * expansion\n",
    "        self.linear1 = nn.Sequential(nn.Conv2d(self.dim1, self.dim2, 1, 1, 0))\n",
    "        self.drop1 = nn.Dropout(drop, inplace=True)\n",
    "        self.act = nn.GELU()\n",
    "        self.linear2 = nn.Sequential(nn.Conv2d(self.dim2, self.dim1, 1, 1, 0),)\n",
    "        self.drop2 = nn.Dropout(drop, inplace=True)\n",
    "        self.dwc = nn.Conv2d(self.dim2, self.dim2, 3, 1, 1, groups=self.dim2)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        x = self.linear1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = x + self.dwc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f54a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape = torch.Size([1, 197, 384])\n",
      "Attention shape = torch.Size([1, 6, 197, 197])\n"
     ]
    }
   ],
   "source": [
    "# attention_module = Attention(dim = embed_dim, num_heads = num_heads, attn_drop =0.0, proj_drop =0.0 )\n",
    "# attn_output, attn = attention_module(pos_embed_output)\n",
    "# print(f\"Attention output shape = {attn_output.shape}\")\n",
    "# print(f\"Attention shape = {attn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6694e",
   "metadata": {},
   "source": [
    "### Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d70195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerStage(nn.Module):\n",
    "    def __init__(self, fmap_size, dim_in, dim_embed, depths, stage_spec, n_groups, heads, stride,\n",
    "                 attn_drop, proj_drop, expansion, drop, drop_path_rate, \n",
    "                 ksize, nat_ksize):\n",
    "\n",
    "        super().__init__()\n",
    "        fmap_size = to_2tuple(fmap_size)\n",
    "        self.depths = depths\n",
    "        hc = dim_embed // heads\n",
    "        self.proj = nn.Conv2d(dim_in, dim_embed, 1, 1, 0) if dim_in != dim_embed else nn.Identity()\n",
    "        self.stage_spec = stage_spec\n",
    "\n",
    "        self.layer_norms = nn.ModuleList([nn.Identity() for d in range(2 * depths)])\n",
    "\n",
    "        self.mlps = nn.ModuleList([ TransformerMLPWithConv(dim_embed, expansion, drop) for _ in range(depths) ])\n",
    "        self.attns = nn.ModuleList()\n",
    "        self.drop_path = nn.ModuleList()\n",
    "        self.layer_scales = nn.ModuleList([ nn.Identity()  for _ in range(2 * depths)])\n",
    "        self.local_perception_units = nn.ModuleList([ nn.Conv2d(dim_embed, dim_embed, kernel_size=3, stride=1, padding=1, groups=dim_embed)\n",
    "                for _ in range(depths)\n",
    "            ])\n",
    "\n",
    "        for i in range(depths):\n",
    "            if stage_spec[i] == 'D':\n",
    "                self.attns.append(\n",
    "                    DAttentionBaseline(fmap_size, fmap_size, heads, \n",
    "                    hc, n_groups, attn_drop, proj_drop, \n",
    "                    stride, ksize)\n",
    "                )\n",
    "            elif stage_spec[i] == 'N':\n",
    "                self.attns.append(\n",
    "                    NeighborhoodAttention2D(dim_embed, nat_ksize, heads, attn_drop, proj_drop)\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(f'Spec: {stage_spec[i]} is not supported.')\n",
    "\n",
    "            self.drop_path.append(DropPath(drop_path_rate[i]) if drop_path_rate[i] > 0.0 else nn.Identity())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        for d in range(self.depths):            \n",
    "            x0 = x\n",
    "            x = self.local_perception_units[d](x.contiguous())\n",
    "            x = x + x0\n",
    "\n",
    "            x0 = x\n",
    "            x, _, _ = self.attns[d](self.layer_norms[2 * d](x))\n",
    "            x = self.layer_scales[2 * d](x)\n",
    "            x = self.drop_path[d](x) + x0\n",
    "            x0 = x\n",
    "            x = self.mlps[d](self.layer_norms[2 * d + 1](x))\n",
    "            x = self.layer_scales[2 * d + 1](x)\n",
    "            x = self.drop_path[d](x) + x0\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557eca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer block output shape = torch.Size([1, 197, 384])\n"
     ]
    }
   ],
   "source": [
    "# transformer_block = Block(dim=embed_dim, num_heads=3, mlp_ratio=4)\n",
    "# transformer_block_output, _ = transformer_block(pos_embed_output)\n",
    "# print(f\"Transformer block output shape = {transformer_block_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f990e91",
   "metadata": {},
   "source": [
    "### Putting it altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d4d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=4, num_classes=1000, expansion=4,\n",
    "                 dim_stem=64, dims=[64, 128, 256, 512], depths=[2, 4, 18, 2], \n",
    "                 heads=[2, 4, 8, 16], drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.2, \n",
    "                 strides=[8, 4, 2, 1],\n",
    "                 stage_spec=[['N', 'D'], ['N', 'D', 'N', 'D'], \n",
    "                             ['N', 'D', 'N', 'D', 'N', 'D', 'N', 'D', 'N', 'D', 'N', 'D', 'N', 'D', 'N', 'D', 'N', 'D'], ['L', 'D']], \n",
    "                 groups=[1, 2, 4, 8],\n",
    "                #  use_pes=[True, True, True, True], \n",
    "                #  dwc_pes=[False, False, False, False],\n",
    "                #  fixed_pes=[False, False, False, False],\n",
    "                #  no_offs=[False, False, False, False],\n",
    "                #  use_dwc_mlps=[True, True, True, True],\n",
    "                 ksizes=[9, 7, 5, 3],\n",
    "                #  ksize_qnas=[3, 3, 3, 3],\n",
    "                #  nqs=[2, 2, 2, 2],\n",
    "                #  qna_activation='exp',\n",
    "                 nat_ksizes=[7, 7, 7, 7],\n",
    "                #  layer_scale_values=[-1,-1,-1,-1],\n",
    "                #  use_lpus=[True, True, True, True]\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.patch_proj = nn.Sequential(\n",
    "            nn.Conv2d(3, dim_stem // 2, 3, patch_size // 2, 1),\n",
    "            LayerNormProxy(dim_stem // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(dim_stem // 2, dim_stem, 3, patch_size // 2, 1),\n",
    "            LayerNormProxy(dim_stem)\n",
    "        )\n",
    "\n",
    "        img_size = img_size // patch_size\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            dim1 = dim_stem if i == 0 else dims[i - 1] * 2\n",
    "            dim2 = dims[i]\n",
    "            self.stages.append(\n",
    "                TransformerStage(\n",
    "                    img_size, dim1, dim2, depths[i],\n",
    "                    stage_spec[i], groups[i], heads[i], strides[i],\n",
    "                    attn_drop_rate, drop_rate, expansion, drop_rate,\n",
    "                    dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
    "                    ksizes[i], nat_ksizes[i])\n",
    "                )\n",
    "            img_size = img_size // 2\n",
    "\n",
    "        self.down_projs = nn.ModuleList()\n",
    "        for i in range(3):\n",
    "            self.down_projs.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(dims[i], dims[i + 1], 3, 2, 1, bias=False),\n",
    "                    LayerNormProxy(dims[i + 1])\n",
    "                ) \n",
    "            )\n",
    "\n",
    "        self.cls_norm = LayerNormProxy(dims[-1]) \n",
    "        self.cls_head = nn.Linear(dims[-1], num_classes)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for m in self.parameters():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def load_pretrained(self, state_dict, lookup_22k):\n",
    "\n",
    "        new_state_dict = {}\n",
    "        for state_key, state_value in state_dict.items():\n",
    "            keys = state_key.split('.')\n",
    "            m = self\n",
    "            for key in keys:\n",
    "                if key.isdigit():\n",
    "                    m = m[int(key)]\n",
    "                else:\n",
    "                    m = getattr(m, key)\n",
    "            if m.shape == state_value.shape:\n",
    "                new_state_dict[state_key] = state_value\n",
    "            else:\n",
    "                # Ignore different shapes\n",
    "                if 'relative_position_index' in keys:\n",
    "                    new_state_dict[state_key] = m.data\n",
    "                if 'q_grid' in keys:\n",
    "                    new_state_dict[state_key] = m.data\n",
    "                if 'reference' in keys:\n",
    "                    new_state_dict[state_key] = m.data\n",
    "                # Bicubic Interpolation\n",
    "                if 'relative_position_bias_table' in keys:\n",
    "                    n, c = state_value.size()\n",
    "                    l_side = int(math.sqrt(n))\n",
    "                    assert n == l_side ** 2\n",
    "                    L = int(math.sqrt(m.shape[0]))\n",
    "                    pre_interp = state_value.reshape(1, l_side, l_side, c).permute(0, 3, 1, 2)\n",
    "                    post_interp = F.interpolate(pre_interp, (L, L), mode='bicubic')\n",
    "                    new_state_dict[state_key] = post_interp.reshape(c, L ** 2).permute(1, 0)\n",
    "                if 'rpe_table' in keys:\n",
    "                    c, h, w = state_value.size()\n",
    "                    C, H, W = m.data.size()\n",
    "                    pre_interp = state_value.unsqueeze(0)\n",
    "                    post_interp = F.interpolate(pre_interp, (H, W), mode='bicubic')\n",
    "                    new_state_dict[state_key] = post_interp.squeeze(0)\n",
    "                if 'cls_head' in keys:\n",
    "                    new_state_dict[state_key] = state_value[lookup_22k]\n",
    "\n",
    "        msg = self.load_state_dict(new_state_dict, strict=False)\n",
    "        return msg\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_proj(x)\n",
    "        for i in range(4):\n",
    "            x = self.stages[i](x)\n",
    "            if i < 3:\n",
    "                x = self.down_projs[i](x)\n",
    "        x = self.cls_norm(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.cls_head(x)\n",
    "        return x, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "076307f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit = VisionTransformer(img_size=img_size, patch_size=patch_size, in_chans=3, \n",
    "#                         embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=4)\n",
    "\n",
    "# vit_output, attn_map = vit(input_data)\n",
    "# vit_output.shape, attn_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1f981",
   "metadata": {},
   "source": [
    "## Visualize Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c8134",
   "metadata": {},
   "source": [
    "We'll load a pretrained model, and visualized the output on sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2871108a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpt = torch.load('/home/surya/Downloads/dino_deitsmall16_pretrain_full_checkpoint.pth')['student']\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in checkpt.items()}\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01d55e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {k:v for k, v in state_dict.items() if 'head.' not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb45ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# with open('../assets/test_image.jpg', 'rb') as f:\n",
    "#     img = Image.open(f)\n",
    "#     img = img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d232d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms as pth_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a90a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = pth_transforms.Compose([\n",
    "#     pth_transforms.Resize([224, 224]),\n",
    "#     pth_transforms.ToTensor(),\n",
    "#     pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "# ])\n",
    "# img = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68100a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5de4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make the image divisible by the patch size\n",
    "# w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n",
    "# img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "# w_featmap = img.shape[-2] // patch_size\n",
    "# h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "# _, attentions = vit(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf3224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dd24be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nh = attentions.shape[1] # number of head\n",
    "\n",
    "# # we keep only the output patch attention\n",
    "# attentions = attentions[0, :, 0, 1:].reshape(nh, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8604d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "285b4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "# attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a16db245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10b1bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73a2877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59a94374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4244e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c4f494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save attentions heatmaps\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=True, scale_each=True), os.path.join(output_dir, \"img.png\"))\n",
    "# for j in range(nh):\n",
    "#     fname = os.path.join(output_dir, \"attn-head\" + str(j) + \".png\")\n",
    "#     plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
    "#     print(f\"{fname} saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cab6a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=True, scale_each=True), os.path.join(args.output_dir, \"img.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cd9671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # save attentions heatmaps\n",
    "#     os.makedirs(args.output_dir, exist_ok=True)\n",
    "#     torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=True, scale_each=True), os.path.join(args.output_dir, \"img.png\"))\n",
    "#     for j in range(nh):\n",
    "#         fname = os.path.join(args.output_dir, \"attn-head\" + str(j) + \".png\")\n",
    "#         plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
    "#         print(f\"{fname} saved.\")\n",
    "\n",
    "#     if args.threshold is not None:\n",
    "#         image = skimage.io.imread(os.path.join(args.output_dir, \"img.png\"))\n",
    "#         for j in range(nh):\n",
    "#             display_instances(image, th_attn[j], fname=os.path.join(args.output_dir, \"mask_th\" + str(args.threshold) + \"_head\" + str(j) +\".png\"), blur=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec617517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66359bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fefa93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aafb90b",
   "metadata": {},
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
